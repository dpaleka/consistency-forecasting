{"P":{"id":"45c47d69-e43a-4744-8734-195a99790e5e","title":"Will Washington D.C. become a state before January 20, 2025?","body":"Resolution Criteria\nThis question will resolve as Yes if legislation making some portion of Washington, DC a new state becomes law before January 20, 2025. The legislation need not go into effect by that date. This question will resolve as No if DC's status remains unchanged, if DC is retroceded to Maryland, or if DC is otherwise granted some, but not all rights afforded to a state\n","resolution_date":"2025-01-19T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"516276d6-c1c1-4d77-8d9f-49881dd0e188","title":"In the 2024 US presidential election, will any state refuse to certify their election results?","body":"Resolution Criteria\nThe question will resolve positively if any state (or DC) does not certify their results by the \"safe harbor\" deadline. The \"safe harbor\" deadline for the 2024 presidential election will be December 10th","resolution_date":"2024-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"dc203ada-cabc-4ebb-8fc8-e526357042d0","title":"Will both of the following occur: (1) Washington D.C. becomes a state before January 20, 2025, and (2) any state refuses to certify their election results by the safe harbor deadline in the 2024 US presidential election?","body":"Resolution Criteria\nThis combined question will resolve as Yes if both of the following conditions are met: (1) Legislation making some portion of Washington, DC a new state becomes law before January 20, 2025. The legislation need not go into effect by that date. This question will resolve as No if DC's status remains unchanged, if DC is retroceded to Maryland, or if DC is otherwise granted some, but not all rights afforded to a state. (2) Any state (or DC) does not certify their results by the \"safe harbor\" deadline, which for the 2024 presidential election will be December 10th.","resolution_date":"2025-01-19T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"42b72c2a-1d15-454d-a653-63a613c9bc49","title":"Will either of the following occur before their respective deadlines?","body":"(a) Legislation making some portion of Washington, DC a new state becomes law before January 20, 2025. The legislation need not go into effect by that date. This will resolve as Yes if such legislation is passed, and as No if DC's status remains unchanged, if DC is retroceded to Maryland, or if DC is otherwise granted some, but not all rights afforded to a state.\n(b) Any state (or DC) does not certify their results by the \"safe harbor\" deadline of December 10th for the 2024 US presidential election.","resolution_date":"2025-01-19T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"32487655-5854-4ac6-bcdc-4c8e95936d92","title":"Will Washington D.C. become a state before January 20, 2025?","body":"Resolution Criteria\nThis question will resolve as Yes if legislation making some portion of Washington, DC a new state becomes law before January 20, 2025. The legislation need not go into effect by that date. This question will resolve as No if DC's status remains unchanged, if DC is retroceded to Maryland, or if DC is otherwise granted some, but not all rights afforded to a state\n","resolution_date":"2025-01-19T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/6218","metadata":{"topics":[],"background_info":"Residents of Washington DC have long been frustrated by a lack of (voting) representation in the US Congress, as well as local governance problems arising from the city's status as a federal district.\nMembers of Congress have in the past introduced legislation intending to convert much of the current federal district into a new state. President-elect Biden has in the past vocalized support for such resolutions.\nThere are numerous proposals for the exact details of such a transition, most including a prominent 'rump federal district' around the US Capitol building."},"resolution":null},"Q":{"id":"e905f488-5695-4c7d-aaa1-68199ca4f7b0","title":"In the 2024 US presidential election, will any state refuse to certify their election results?","body":"Resolution Criteria\nThe question will resolve positively if any state (or DC) does not certify their results by the \"safe harbor\" deadline. The \"safe harbor\" deadline for the 2024 presidential election will be December 10th","resolution_date":"2024-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/6944","metadata":{"topics":[],"background_info":"Recent controversy over state election laws has caused some Democrats to be concerned that Republicans intend to use their political power in state governments to distort future elections in their favor. One commentator recently suggested:\n\"If a Democrat wins a GOP-controlled swing state in 2024 … there’s a very good chance the victory isn’t certified\"\nElection certification is the process in which states confirm the election results and declare them to be the official results. Typically states set their own deadlines for election certification, which in 2020 ranged from November 5th to December 8th for the presidential contest (several states appear to have no deadline), but federal law provides a \"safe harbor\" deadline by which states must formally certify their election results in order for the certified results to be federally recognized as governing the outcome. The \"safe harbor\" deadline is set by 3 U.S. Code § 5 and 3 U.S. Code § 7 which sets the safe harbor deadline as six days prior to the first Monday after the second Wednesday in December of the election year.\n"},"resolution":null}},"relevance":{"reasons":["Political climate: The political climate that would allow for Washington D.C. to become a state might also be one where election results are more contentious, leading to a state refusing to certify their results.","Legislative priorities: If the political party in power is pushing for D.C. statehood, they might also be involved in disputes over election certifications, especially if the election results are close or contested.","Polarization: High levels of political polarization could lead to both the push for D.C. statehood and disputes over election certifications."],"conclusion":"There are plausible reasons to consider the logical combination of these questions, as they both involve significant political actions that could be influenced by the same underlying factors.","score":7.0}}}
{"P":{"id":"49a4d4c5-322f-458b-95f2-476a16e857ff","title":"Will Robin Hanson win his bet against Matthew Barnett on whether ems will come before de novo AGI?","body":"Resolution Criteria\nThis question resolves positively in the event that Matthew Barnett (or his descendants) publicly concedes the bet to Robin Hanson, and resolves negatively in the event that Robin Hanson (or his descendants) publicly concedes to Matthew Barnett. In the event that one party declares victory but the other party does not concede, Metaculus admins will use their discretion in resolving the bet.\nRelated question: [When will the US labor force participation rate fall below 10%?](https://www.metaculus.com/questions/8480/us-l…\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"00530d3e-c6f4-4b0e-8ae8-e1e44e1dc6a7","title":"Will a US Census in 2070 or earlier show a decline in population?","body":"Resolution Criteria\nThis question will resolve as Yes if a Census conducted before 2071 shows a drop in US population over the previous Census, according to official statistics from the United States Census Bureau. If the Census Bureau changes its name or merges into another entity, those statistics will be used instead","resolution_date":"2069-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"f07d466c-bc5b-4fee-ba79-a75e0aef1fca","title":"Will both of the following occur: (a) Robin Hanson wins his bet against Matthew Barnett on whether ems will come before de novo AGI, and (b) a US Census in 2070 or earlier shows a decline in population?","body":"Resolution Criteria for (a):\nThis question resolves positively in the event that Matthew Barnett (or his descendants) publicly concedes the bet to Robin Hanson, and resolves negatively in the event that Robin Hanson (or his descendants) publicly concedes to Matthew Barnett. In the event that one party declares victory but the other party does not concede, Metaculus admins will use their discretion in resolving the bet.\n\nResolution Criteria for (b):\nThis question will resolve as Yes if a Census conducted before 2071 shows a drop in US population over the previous Census, according to official statistics from the United States Census Bureau. If the Census Bureau changes its name or merges into another entity, those statistics will be used instead.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"35d26fba-fa42-4323-ac6f-b69e1b9527cf","title":"Will either of the following occur:","body":"(a) Robin Hanson wins his bet against Matthew Barnett on whether ems will come before de novo AGI, which resolves positively if Matthew Barnett (or his descendants) publicly concedes the bet to Robin Hanson, and negatively if Robin Hanson (or his descendants) publicly concedes to Matthew Barnett. In the event that one party declares victory but the other party does not concede, Metaculus admins will use their discretion in resolving the bet.\n(b) A US Census conducted in 2070 or earlier shows a decline in population, according to official statistics from the United States Census Bureau. If the Census Bureau changes its name or merges into another entity, those statistics will be used instead.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"a641a2d2-5577-4c2d-87d0-dc55dca1607d","title":"Will Robin Hanson win his bet against Matthew Barnett on whether ems will come before de novo AGI?","body":"Resolution Criteria\nThis question resolves positively in the event that Matthew Barnett (or his descendants) publicly concedes the bet to Robin Hanson, and resolves negatively in the event that Robin Hanson (or his descendants) publicly concedes to Matthew Barnett. In the event that one party declares victory but the other party does not concede, Metaculus admins will use their discretion in resolving the bet.\nRelated question: [When will the US labor force participation rate fall below 10%?](https://www.metaculus.com/questions/8480/us-l…\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8475","metadata":{"topics":[],"background_info":"On Twitter Robin Hanson proposed,\nOK, so to summarize a proposal: I'd bet my $1K to your $9K (both increased by S&P500 scale factor) that when US labor participation rate < 10%, em-like automation will contribute more to GDP than AGI-like. And we commit our descendants to the bet.\nMatthew Barnett replied,\nI agree to this bet."},"resolution":null},"Q":{"id":"48b83156-fa49-45dd-94ce-4fd2329c122a","title":"Will a US Census in 2070 or earlier show a decline in population?","body":"Resolution Criteria\nThis question will resolve as Yes if a Census conducted before 2071 shows a drop in US population over the previous Census, according to official statistics from the United States Census Bureau. If the Census Bureau changes its name or merges into another entity, those statistics will be used instead","resolution_date":"2069-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/7857","metadata":{"topics":[],"background_info":"The United States Census takes a census every 10 years. In the most recent census in 2020, the US population had grown by 7.4% in the past decade. This is contrast to some developed nations whose populations are declining, though population growth in the United States has slowed and the 7.4% is one of the lowest-ever decade-on-decade growth rates ever recorded. Recently, US fertility has hit record lows though it is still not as low as countries like Japan, Italy, Spain, and South Korea. Immigration to the United States is a significant source of population growth, especially in contrast with countries like Japan."},"resolution":null}},"relevance":{"reasons":["The development of ems (emulated minds) or de novo AGI (artificial general intelligence) could have significant societal impacts, including on population trends. For example, if ems become widespread, they might reduce the need for human labor, potentially affecting birth rates and population growth.","Technological advancements like ems or AGI could lead to changes in economic structures, social policies, and even migration patterns, all of which could influence population trends.","The outcome of the bet between Robin Hanson and Matthew Barnett could be seen as a proxy for the timeline of significant technological advancements, which in turn could impact long-term demographic trends."],"conclusion":"There are plausible reasons to consider the logical combination of these questions, as the development of ems or AGI could have far-reaching effects on population trends.","score":7.0}}}
{"P":{"id":"0fb10710-9d27-4b78-a159-7a215588de4b","title":"Will an inorganic nanoparticle-enabled cancer therapy be approved by the FDA before 2041?","body":"Resolution Criteria\nThis question will resolve as Yes if, before January 1, 2041, the FDA gives full approval to a cancer therapy that uses inorganic nanoparticles in the patient's treatment. The therapy must target the cancer cells rather than downstream symptoms. We specify \"inorganic\" to exclude lipid nanoparticles etc\n","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"950e689d-4886-4b28-bd09-434b7fbdb67c","title":"By 2050, Will at least 20% of US births be screened as embryos to detect genetic disorders or disabilities?","body":"Resolution Criteria\nThis question will resolve as Yes if at least 20% of babies born in the United States in any year before 2050 underwent embryo screening or PGD, according to credible media reports, statements by the US government, or public health agencies. Screening must have taken place before the embryo was implanted in the uterus\n","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"dfe28386-a5f3-4e04-9219-53ff0b6db462","title":"Will both of the following occur before their respective deadlines?","body":"(1) An inorganic nanoparticle-enabled cancer therapy is approved by the FDA before 2041, and (2) at least 20% of US births are screened as embryos to detect genetic disorders or disabilities before 2050.\n\n**Resolution Criteria**\n- **For the first part**: This question will resolve as Yes if, before January 1, 2041, the FDA gives full approval to a cancer therapy that uses inorganic nanoparticles in the patient's treatment. The therapy must target the cancer cells rather than downstream symptoms. The term \"inorganic\" is specified to exclude lipid nanoparticles and similar materials.\n- **For the second part**: This question will resolve as Yes if at least 20% of babies born in the United States in any year before 2050 underwent embryo screening or PGD, according to credible media reports, statements by the US government, or public health agencies. Screening must have taken place before the embryo was implanted in the uterus.","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"0dfe5236-80af-44f7-9804-bc34322d7bc7","title":"Will either of the following occur before their respective deadlines?","body":"(a) An inorganic nanoparticle-enabled cancer therapy is approved by the FDA before 2041. This therapy must target the cancer cells directly, using inorganic nanoparticles, excluding lipid nanoparticles and similar materials. (b) At least 20% of US births in any year before 2050 are screened as embryos to detect genetic disorders or disabilities, with screening occurring before the embryo is implanted in the uterus. Resolution for (a) will be based on FDA's full approval of such a therapy before January 1, 2041. Resolution for (b) will be based on credible media reports, statements by the US government, or public health agencies confirming that the screening rate reached or exceeded 20% in any year before 2050.","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"3219eea2-97e4-498d-80ad-1952cd7a1704","title":"Will an inorganic nanoparticle-enabled cancer therapy be approved by the FDA before 2041?","body":"Resolution Criteria\nThis question will resolve as Yes if, before January 1, 2041, the FDA gives full approval to a cancer therapy that uses inorganic nanoparticles in the patient's treatment. The therapy must target the cancer cells rather than downstream symptoms. We specify \"inorganic\" to exclude lipid nanoparticles etc\n","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/7994","metadata":{"topics":[],"background_info":"This question supports and is linked to a fortified essay on the future of nanotechnology by Physical Chemist Kevin Ausman. Click here to read the full essay. There is a related question, whether nanoparticle-enabled cancer therapy will be approved by 2031, here.\nInorganic nanoparticles are small collections of a few thousand to a few billion atoms, typically ranging from 1 to 100 nm in diameter. They have many peculiar chemical and physical properties, which make them attractive for designing therapies.\nAn example of such technology is called Aurolase Therapy, and has been in development for two decades. It's now in pilot studies in humans with early results giving reason for optimism."},"resolution":null},"Q":{"id":"46cca972-9085-4415-a58f-b1a9ed884716","title":"By 2050, Will at least 20% of US births be screened as embryos to detect genetic disorders or disabilities?","body":"Resolution Criteria\nThis question will resolve as Yes if at least 20% of babies born in the United States in any year before 2050 underwent embryo screening or PGD, according to credible media reports, statements by the US government, or public health agencies. Screening must have taken place before the embryo was implanted in the uterus\n","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8505","metadata":{"topics":[],"background_info":"Embryo screening is the process of examining the genome of an embryo to determine if certain genes or sets of genes are present (the technical term is preimplantation genetic testing (PGD)). Examining embryos for genetic defects can allow prospective parents to select embryos without genetic defects to implant and carry to term. Currently this process is most prevalently used during in vitro fertilization (IVF), where eggs are fertilized by sperm in a lab to create the embryo before being implanted into the uterus. IVF is generally intended to help parents who have trouble conceiving or may be at elevated risk of birth defects.\nSome expect embryo screening for health defects to gain in popularity in the future. Neuroscientist and fiction author Erik Hoel wrote the following about embryo screening in the year 2050 in a blog post published August 25, 2021\nSex and reproduction will become even more separated, and screening multiple embryos for their health before implantation will be common, although not universal.\nAccording to the National Center for Health Statistics there were 3.75 million babies born in the US in 2019 and 3.6 million in 2020. According to the CDC, approximately 1.9% of infants born each year were conceived using assisted reproductive technology (ART), the main type of which is IVF. One study estimates that 4% to 6% of all IVF cycles in the US used PGD.\n"},"resolution":null}},"relevance":{"reasons":["Both questions pertain to advancements in medical technology and healthcare, which are areas of significant interest for public health, policy, and investment.","The approval of an inorganic nanoparticle-enabled cancer therapy could indicate a broader trend in the acceptance and integration of advanced medical technologies, which might also influence the adoption of genetic screening technologies.","Both questions involve regulatory approval and public acceptance, which are interconnected factors in the healthcare industry."],"conclusion":"The logical combination of these questions is relevant and could provide insights into the future landscape of medical technology and healthcare.","score":8.0}}}
{"P":{"id":"89fefe45-d72f-4e9c-8791-43ff353e9212","title":"Will there be a US-China war before 2035?","body":"Resolution Criteria\nFor the purposes of this question, a US-China war is defined as the US and China collectively suffering at least 1,000 battle-related deaths in conflicts with each other in a single calendar year, as reported by credible news, government, or multi-national sources. Deaths in battles fought between the US and an ally of China or between China and an ally of the US will not count towards positive resolution. If this does not occur before January 1, 2035, this question will resolve negatively.\nWe here define battle related deaths as defined by the Uppsala University Department of Peace and Conflict Studies.\nResolution will come from reputable news sources, from official federal or military announcements, or from multinational institutions like the UN or NATO\n","resolution_date":"2034-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"9b5f3a56-dd5b-426c-901f-6f60079ad795","title":"By 2100, will the human population decrease by at least 10% during any period of 5 years?","body":"Resolution Criteria\nThis question will resolve as Yes if the human population (on Earth, and possibly elsewhere) decreases by at least 10% in any period of 5 years or less. Years are here defined as consecutive calendar years.\nThis question is part of the Ragnarök Question Series. Check out the other questions in the series:\nIf a global biological catastrophe occurs, will it reduce the human population by 95% or more?\nIf an artificial intelligence catastrophe occurs, will it reduce the human population by 95% or more?\nIf a nuclear catastrophe occurs, will it reduce the human population by 95% or more?\nIf a global climate disaster occurs by 2100, will the human population decline by 95% or more?\nIf a global nanotechnology catastrophe occurs by 2100, will the human population decline by 95% or more?\nAlso, please check out our questions on whether a global catastrophe will occur by 2100, and if so, which?:\nBy 2100 will the human population decrease by at least 10% during any period of 5 years?\nWill such a catastrophe be due to either human-made climate change or geoengineering?\nWill such a catastrophe be due to a nanotechnology failure-mode?\nWill such a catastrophe be due to nuclear war?\nWill such a catastrophe be due to an artificial intelligence failure-mode?\nWill such a catastrophe be due to biotechnology or bioengineered organisms?\nAll results are analysed here, and will be updated periodically.\n</small\n","resolution_date":"2100-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"88a993f0-3966-484f-a242-71449333862e","title":"Will both of the following occur:","body":"(1) A US-China war, as defined by the US and China collectively suffering at least 1,000 battle-related deaths in conflicts with each other in a single calendar year before January 1, 2035. Deaths in battles fought between the US and an ally of China or between China and an ally of the US will not count towards positive resolution. Battle-related deaths are as defined by the Uppsala University Department of Peace and Conflict Studies. Resolution will come from reputable news sources, from official federal or military announcements, or from multinational institutions like the UN or NATO.\n(2) A decrease in the human population by at least 10% during any period of 5 years or less by the year 2100. This includes the human population on Earth and possibly elsewhere, with years defined as consecutive calendar years. This question is part of the Ragnarök Question Series, which includes various scenarios such as global biological, artificial intelligence, nuclear, climate, and nanotechnology catastrophes potentially leading to significant population declines.","resolution_date":"2100-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"893792e8-f2ed-444a-a378-703f220da0b9","title":"Will either of the following occur before their respective deadlines?","body":"(a) A US-China war, defined as the US and China collectively suffering at least 1,000 battle-related deaths in conflicts with each other in a single calendar year, as reported by credible news, government, or multi-national sources, before January 1, 2035. Deaths in battles fought between the US and an ally of China or between China and an ally of the US will not count towards positive resolution. Battle related deaths are as defined by the Uppsala University Department of Peace and Conflict Studies. Resolution will come from reputable news sources, from official federal or military announcements, or from multinational institutions like the UN or NATO.\n(b) A decrease in the human population by at least 10% during any period of 5 years or less, as defined by consecutive calendar years, before the year 2100. This question is part of the Ragnarök Question Series, which includes various scenarios such as global biological, artificial intelligence, nuclear, climate, and nanotechnology catastrophes potentially leading to significant population declines.","resolution_date":"2100-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"de293c98-f319-498a-9737-c482a204bfa3","title":"Will there be a US-China war before 2035?","body":"Resolution Criteria\nFor the purposes of this question, a US-China war is defined as the US and China collectively suffering at least 1,000 battle-related deaths in conflicts with each other in a single calendar year, as reported by credible news, government, or multi-national sources. Deaths in battles fought between the US and an ally of China or between China and an ally of the US will not count towards positive resolution. If this does not occur before January 1, 2035, this question will resolve negatively.\nWe here define battle related deaths as defined by the Uppsala University Department of Peace and Conflict Studies.\nResolution will come from reputable news sources, from official federal or military announcements, or from multinational institutions like the UN or NATO\n","resolution_date":"2034-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8362","metadata":{"topics":[],"background_info":"Beginning with China's entry to the nuclear club in the 1960s and culminating most recently with the development of China's naval power in the South China Sea and the threats to Taiwan, the US and China have experienced increasingly tense foreign relations over the last several decades. In addition, the US and China recently entered into a trade war in 2018 at the behest of the former President Trump. Questions now arise on the potential for a US-China cold war, or military conflict in the South China Sea."},"resolution":null},"Q":{"id":"fef51ad0-c28a-48ec-8486-0879708a391d","title":"By 2100, will the human population decrease by at least 10% during any period of 5 years?","body":"Resolution Criteria\nThis question will resolve as Yes if the human population (on Earth, and possibly elsewhere) decreases by at least 10% in any period of 5 years or less. Years are here defined as consecutive calendar years.\nThis question is part of the Ragnarök Question Series. Check out the other questions in the series:\nIf a global biological catastrophe occurs, will it reduce the human population by 95% or more?\nIf an artificial intelligence catastrophe occurs, will it reduce the human population by 95% or more?\nIf a nuclear catastrophe occurs, will it reduce the human population by 95% or more?\nIf a global climate disaster occurs by 2100, will the human population decline by 95% or more?\nIf a global nanotechnology catastrophe occurs by 2100, will the human population decline by 95% or more?\nAlso, please check out our questions on whether a global catastrophe will occur by 2100, and if so, which?:\nBy 2100 will the human population decrease by at least 10% during any period of 5 years?\nWill such a catastrophe be due to either human-made climate change or geoengineering?\nWill such a catastrophe be due to a nanotechnology failure-mode?\nWill such a catastrophe be due to nuclear war?\nWill such a catastrophe be due to an artificial intelligence failure-mode?\nWill such a catastrophe be due to biotechnology or bioengineered organisms?\nAll results are analysed here, and will be updated periodically.\n</small\n","resolution_date":"2100-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/1493","metadata":{"topics":[],"background_info":"You can now see an excellent visualization of global catastrophic risks estimates produced in the Ragnarök series here.\nIt’s dangerous to be alive and risks are everywhere. But not all risks are created equally. Those that are especially large in scope and severe in intensity are global catastrophic risks, which are risks that could inflict serious damage to human well-being on a global scale.\nUntil relatively recently, most global catastrophic risks were natural, such as the supervolcano episodes and asteroidal/cometary impacts that led to mass extinctions millions of years ago. Other natural risks might include a pandemic of naturally occurring disease, non-anthropogenic climate change, supernovae, gamma-ray bursts, and spontaneous decay of cosmic vacuum state. Humanity has survived these natural existential risks for hundreds of thousands of years; which suggests that it is not any of these that will do us in within the next hundred.\nBy contrast, through technological advances, our species is introducing entirely new kinds of risks, anthropogenic risks, which are man-made threats that have no track record of surviving. Our longevity as a species therefore offers no strong prior grounds for confident optimism. Examples of anthropogenic risks are nuclear war, advanced artificial intelligence, biotechnology and bioengineered organisms, human-made climate change and nanotechnology risks.\nThere are two complementary ways of estimating the chances of catastrophe. What we could call the direct way is to analyze the various specific failure-modes, assign them probabilities, which is what--at least partially-- the questions in the Ragnarök series are designed to do.\nSecondly, there is the indirect way. As Nick Bostrom has argued, there are theoretical constraints that can be brought to bear on the issue, based on some general features of the world in which we live. There is only small number of these, but they are important because they do not rely on making a lot of guesses about the details of future technological and social developments. For example, the so-called Doomsday argument, which purports to show that we have systematically underestimated the probability that humankind will go extinct relatively soon.\nMoreover, the Fermi Paradox tells us that it is not the case that life evolves on a significant fraction of Earth-like planets and proceeds to develop advanced technology. Hence, there must be (at least) one Great Filter – an evolutionary step that is extremely improbable – somewhere on the line between Earth-like planet and colonizing-in-detectable-ways civilization. If the Great Filter isn’t in our past, we must fear it in our (near) future.\n"},"resolution":null}},"relevance":{"reasons":["A US-China war could potentially escalate into a larger global conflict, which might have severe consequences for the global population, including significant loss of life and disruption of societal structures.","A major war between two superpowers like the US and China could lead to economic collapse, resource shortages, and other cascading effects that might contribute to a significant population decline.","The use of advanced weaponry, including nuclear weapons, in a US-China war could result in catastrophic loss of life and long-term environmental damage, potentially leading to a population decrease."],"conclusion":"The logical combination of these questions is worth considering because a US-China war could have far-reaching consequences that might contribute to a significant population decline.","score":8.0}}}
{"P":{"id":"276259ad-8665-42f5-94f1-d67ca13b1b6c","title":"If human-level artificial intelligence is developed, will World GDP grow by at least 30.0% in any of the subsequent 15 years?","body":"Resolution Criteria\nIf a machine passes the test in this Metaculus question, this question resolves as YES if world GDP grows by at least 30.0% for any single year in the fifteen years following the date of the test, according to a reputable organization such as The World Bank, and NO if it doesn't.\nIf no machine passes that test by 2100-01-01, then this question resolves ambiguously.\n","resolution_date":"2099-12-31T00:00:00Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"78434bfb-d063-4235-a6c6-f48ba0914a00","title":"Will there be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles?","body":"Resolution Criteria\nResolution is by credible assessment of world GDP in constant US dollars (or other appropriately widely-accepted units).\nThere will be an ambiguous resolution if there is no 4 year (nor 1 year) doubling interval by 2050, to isolate specifically the takeoff speed from other things like the chances of no takeoff occurring at all or human extinction\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"01fe12e0-f575-4646-82c8-7cbb7ffcfc2b","title":"Will both of the following occur: (1) human-level artificial intelligence is developed and subsequently, World GDP grows by at least 30.0% in any of the subsequent 15 years, and (2) there is a complete 4 year interval in which world output doubles before the first 1 year interval in which world output doubles?","body":"Resolution Criteria for (1): If a machine passes the test in this Metaculus question, this question resolves as YES if world GDP grows by at least 30.0% for any single year in the fifteen years following the date of the test, according to a reputable organization such as The World Bank, and NO if it doesn't. If no machine passes that test by 2100-01-01, then this question resolves ambiguously.\n\nResolution Criteria for (2): Resolution is by credible assessment of world GDP in constant US dollars (or other appropriately widely-accepted units). There will be an ambiguous resolution if there is no 4 year (nor 1 year) doubling interval by 2050, to isolate specifically the takeoff speed from other things like the chances of no takeoff occurring at all or human extinction.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"204e217d-6036-43f0-a26e-6782ce77587d","title":"Will either of the following occur:","body":"(a) If human-level artificial intelligence is developed, will World GDP grow by at least 30.0% in any of the subsequent 15 years? Resolution Criteria: If a machine passes the test in this Metaculus question, this question resolves as YES if world GDP grows by at least 30.0% for any single year in the fifteen years following the date of the test, according to a reputable organization such as The World Bank, and NO if it doesn't. If no machine passes that test by 2100-01-01, then this question resolves ambiguously.\n(b) Will there be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles? Resolution Criteria: Resolution is by credible assessment of world GDP in constant US dollars (or other appropriately widely-accepted units). There will be an ambiguous resolution if there is no 4 year (nor 1 year) doubling interval by 2050, to isolate specifically the takeoff speed from other things like the chances of no takeoff occurring at all or human extinction.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"af9e0602-a1ce-40c4-a117-69e471ccb07c","title":"If human-level artificial intelligence is developed, will World GDP grow by at least 30.0% in any of the subsequent 15 years?","body":"Resolution Criteria\nIf a machine passes the test in this Metaculus question, this question resolves as YES if world GDP grows by at least 30.0% for any single year in the fifteen years following the date of the test, according to a reputable organization such as The World Bank, and NO if it doesn't.\nIf no machine passes that test by 2100-01-01, then this question resolves ambiguously.\n","resolution_date":"2099-12-31T00:00:00Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/3477","metadata":{"topics":[],"background_info":"In economist Robin Hanson's 2001 paper Economic Growth Given Machine Intelligence, he writes\nA simple exogenous growth model gives conservative estimates of the economic implications of machine intelligence. [...] Without machine intelligence, world product grows at a familiar rate of 4.3% per year, doubling every 16 years, with about 40% of technological progress coming from ordinary computers. With machine intelligence, the (instantaneous) annual growth rate would be 45%, ten times higher, making world product double every 18 months! If the product shares are raised by 20%, and general technology growth is lowered to preserve the 4.4% figure, the new doubling time falls to less than 6 months."},"resolution":null},"Q":{"id":"a87c8f8f-1fc3-4f43-9d34-eff5b5676738","title":"Will there be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles?","body":"Resolution Criteria\nResolution is by credible assessment of world GDP in constant US dollars (or other appropriately widely-accepted units).\nThere will be an ambiguous resolution if there is no 4 year (nor 1 year) doubling interval by 2050, to isolate specifically the takeoff speed from other things like the chances of no takeoff occurring at all or human extinction\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/736","metadata":{"topics":[],"background_info":"Related Questions on Metaculus:\nWill a 8-year GWP doubling complete before a 2-year doubling does?\nWill there be a 4 year interval in which world output doubles before 2050?\nAs of 2018, it's taken about 16 years for the world economic output to double. So it might seem absurd to talk about it doubling in one or even four years. But there is a potential major change on the horizon: very advanced Artificial Intelligence.\nAn important consideration in how to approach the AI alignment problem is the speed of the takeoff from massively subhuman AI to massively superhuman AI. The effectiveness of capacity limitation, as well as the existence of fire alarms for AI safety, are heavily dependent on this, for example.\nPaul Christiano suggests operationalising the takeoff speed in terms of economic growth. A slow takeoff is one where the economy doubles in four years before the first time it doubles in one year, and a fast takeoff is one where it does not. (See the same article for Christiano's list of arguments regarding fast vs. slow takeoff.)\n"},"resolution":null}},"relevance":{"reasons":["The development of human-level artificial intelligence (AI) could have a significant impact on global economic growth. If AI reaches a level where it can perform tasks as well as or better than humans, it could lead to substantial increases in productivity, innovation, and efficiency, potentially driving significant GDP growth.","The second question is related to the speed and nature of economic growth following the development of human-level AI. If world output doubles in a 4-year interval before it doubles in a 1-year interval, it could indicate a more gradual and sustained economic growth rather than a sudden, explosive growth. This could be important for understanding the dynamics of AI-driven economic changes and for planning economic policies accordingly."],"conclusion":"The logical combination of these questions is relevant and worth considering, as they both pertain to the economic impact and growth patterns following the development of human-level AI.","score":8.0}}}
{"P":{"id":"2f16a244-f96f-4c0d-a6d5-78be3cc16ce3","title":"If the 2024 US Presidential Election is Trump vs. Biden, will Trump win?","body":"Resolution Criteria\nThis question asks whether, contingent on the 2024 US presidential election being a contest between Democratic nominee Joseph R. Biden and Republican nominee Donald J. Trump, will Donald J. Trump receive at least 270 votes in the electoral college, as certified by Congress in January 2025? If so, this question resolves positively. If the contest is Biden v Trump and Biden wins, this question resolves negatively. If the contest is not Biden v Trump, this question resolves ambiguously\n","resolution_date":"2024-11-05T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"7633ff0d-f6f3-4fd2-bde4-a09804550748","title":"Will US Non-Overseas Contingency operations military deaths exceed 3,000 in any calendar year before 2032?","body":"Resolution Criteria\nThis question will resolve as Yes if the number of military deaths across all branches of the US armed forces is greater than 3,000 in any calendar year between 2019 and 2031 (inclusive), according to the Congressional Research Service. Deaths should include all branches of the US armed forces and all active duty troops, including mobilized Reserve and Guard components. The estimate should include only \"Non-Overseas Contingency Operations\". Contingency Operations are defined by CRS according to\nSection 101 of Title 10, United States Code, defines a contingency operation as any Secretary of Defense-designated military operation “in which members of the armed forces are or may become involved in military actions, operations, or hostilities against an enemy of the United States or against an opposing military force.\nIf data by CRS is not available or complete between 2019 and 2031, Metaculus may select another credible source of data, or resolve as Ambiguous at their discretion\n","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"6f03b63b-d825-4524-88db-e574caac1ed7","title":"Will both of the following occur: (1) Donald J. Trump wins the 2024 US Presidential Election against Joseph R. Biden, and (2) US Non-Overseas Contingency operations military deaths exceed 3,000 in any calendar year before 2032?","body":"Resolution Criteria for (1): This question asks whether, contingent on the 2024 US presidential election being a contest between Democratic nominee Joseph R. Biden and Republican nominee Donald J. Trump, will Donald J. Trump receive at least 270 votes in the electoral college, as certified by Congress in January 2025? If so, this question resolves positively. If the contest is Biden v Trump and Biden wins, this question resolves negatively. If the contest is not Biden v Trump, this question resolves ambiguously.\n\nResolution Criteria for (2): This question will resolve as Yes if the number of military deaths across all branches of the US armed forces is greater than 3,000 in any calendar year between 2019 and 2031 (inclusive), according to the Congressional Research Service. Deaths should include all branches of the US armed forces and all active duty troops, including mobilized Reserve and Guard components. The estimate should include only \"Non-Overseas Contingency Operations\". Contingency Operations are defined by CRS according to Section 101 of Title 10, United States Code, as any Secretary of Defense-designated military operation “in which members of the armed forces are or may become involved in military actions, operations, or hostilities against an enemy of the United States or against an opposing military force.\" If data by CRS is not available or complete between 2019 and 2031, Metaculus may select another credible source of data, or resolve as Ambiguous at their discretion.","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"4796319b-bc89-4649-8514-0191589f9854","title":"Will either of the following occur:","body":"(a) If the 2024 US Presidential Election is Trump vs. Biden, will Trump win? This question resolves positively if, contingent on the 2024 US presidential election being a contest between Democratic nominee Joseph R. Biden and Republican nominee Donald J. Trump, Donald J. Trump receives at least 270 votes in the electoral college, as certified by Congress in January 2025. If the contest is Biden v Trump and Biden wins, this question resolves negatively. If the contest is not Biden v Trump, this question resolves ambiguously.\n(b) Will US Non-Overseas Contingency operations military deaths exceed 3,000 in any calendar year before 2032? This question will resolve as Yes if the number of military deaths across all branches of the US armed forces is greater than 3,000 in any calendar year between 2019 and 2031 (inclusive), according to the Congressional Research Service. Deaths should include all branches of the US armed forces and all active duty troops, including mobilized Reserve and Guard components. The estimate should include only \"Non-Overseas Contingency Operations\". Contingency Operations are defined by CRS according to Section 101 of Title 10, United States Code, as any Secretary of Defense-designated military operation “in which members of the armed forces are or may become involved in military actions, operations, or hostilities against an enemy of the United States or against an opposing military force.\" If data by CRS is not available or complete between 2019 and 2031, Metaculus may select another credible source of data, or resolve as Ambiguous at their discretion.","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"527169fa-5e9c-4c47-82cc-91505b468b36","title":"If the 2024 US Presidential Election is Trump vs. Biden, will Trump win?","body":"Resolution Criteria\nThis question asks whether, contingent on the 2024 US presidential election being a contest between Democratic nominee Joseph R. Biden and Republican nominee Donald J. Trump, will Donald J. Trump receive at least 270 votes in the electoral college, as certified by Congress in January 2025? If so, this question resolves positively. If the contest is Biden v Trump and Biden wins, this question resolves negatively. If the contest is not Biden v Trump, this question resolves ambiguously\n","resolution_date":"2024-11-05T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8483","metadata":{"topics":[],"background_info":"The 2024 United States presidential election will be the 60th quadrennial presidential election, scheduled for Tuesday, November 5, 2024. It will be the first presidential election after electoral votes are redistributed according to the post–2020 census reapportionment. Incumbent president Joe Biden has stated that he intends to run for re-election to a second term, although no official statements of candidacy have been filed as of November 3, 2021.\nFormer president Donald Trump has heavily hinted (see numerous comments on this question) that he plans to seek the presidency in 2024, but has likewise not filed an official statement of candidacy as of November 3, 2021.\nAs of early November 2021, President Biden's approval rating stands at approximately 43% according to FiveThirtyEight. This is the lowest level of approval yet seen in his presidency.\n"},"resolution":null},"Q":{"id":"29ea3d22-896a-4a09-ab97-1969f3ffe165","title":"Will US Non-Overseas Contingency operations military deaths exceed 3,000 in any calendar year before 2032?","body":"Resolution Criteria\nThis question will resolve as Yes if the number of military deaths across all branches of the US armed forces is greater than 3,000 in any calendar year between 2019 and 2031 (inclusive), according to the Congressional Research Service. Deaths should include all branches of the US armed forces and all active duty troops, including mobilized Reserve and Guard components. The estimate should include only \"Non-Overseas Contingency Operations\". Contingency Operations are defined by CRS according to\nSection 101 of Title 10, United States Code, defines a contingency operation as any Secretary of Defense-designated military operation “in which members of the armed forces are or may become involved in military actions, operations, or hostilities against an enemy of the United States or against an opposing military force.\nIf data by CRS is not available or complete between 2019 and 2031, Metaculus may select another credible source of data, or resolve as Ambiguous at their discretion\n","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/3124","metadata":{"topics":[],"background_info":"According to USNI News reporting on the May 20, 2019 Congressional Research Service report:\nSince 2006—five years after the start of major combat operations in Afghanistan and three years after the U.S.-led invasion of Iraq—a total of 16,652 active-duty personnel and mobilized reservists have died while serving in the U.S. Armed Forces. Seventy-three percent of these casualties occurred under circumstances unrelated to war, a category classified in this report as Non-Overseas Contingency Operations, or Non-OCO. Twenty-seven percent have died while serving in OCO operations—primarily within the territory of Iraq and Afghanistan—during periods of active combat operations. OCO operations related to Afghanistan primarily include Enduring Freedom and Freedom’s Sentinel. For Iraq, OCO operations include Iraqi Freedom, New Dawn, and Inherent Resolve. Figure 1 summarizes all service deaths since 2006.\n"},"resolution":null}},"relevance":{"reasons":["A Trump presidency could potentially influence military policies, including engagement rules, training, and safety protocols, which might affect the number of non-overseas contingency operations military deaths.","Changes in military funding, leadership, and strategic priorities under a Trump administration could also impact the rate of military deaths.","Public and political attitudes towards military engagement and safety measures might shift under a Trump administration, potentially influencing the number of military deaths."],"conclusion":"There are plausible reasons to consider the logical combination of these questions, as a Trump presidency could have an impact on military policies and outcomes.","score":7.0}}}
{"P":{"id":"d455ae26-b645-47f8-9372-59c4d4b3eee3","title":"If there's a nuclear conflict involving >100 detonations, will that cause >1 billion fatalities within 10 years?","body":"Resolution Criteria\nRelated Questions on Metaculus:\nIf there's a nuclear conflict involving >1000 detonations, will that cause >4 billion fatalities within 10 years?\nIf there's a nuclear conflict involving >100 detonations, will that cause >1 million fatalities within 1 month?\nIf there's a nuclear conflict involving >1000 detonations, will humanity be extinct within 50 years?\nIf there's a nuclear conflict involving >1000 detonations, will humanity's population be <400 million 50 years later?\nWill the first nuclear conflict involving >100 detonations cause >100m fatalities within 1 month of the final detonation?\nDecisions about how much to prioritize nuclear risk reduction and how best to reduce nuclear risk should be guided in part by our best guesses about:\nhow many deaths would occur given a large-scale nuclear exchange\nwhat proportion of those deaths would occur fairly soon after the detonations (e.g., from the initial blast and fires) rather than later on (e.g., from fallout or nuclear winter effects)\nFor example, this is relevant to the existential risk posed by nuclear weapons and the value of investing in research and development on \"resilient food\".\nSee here, and the sources linked to from there, for previous discussion of these sorts of questions and why they matter.\nIf there's a nuclear conflict involving >100 detonations, will that cause more than 1 billion fatalities within 10 years?\nThis question will resolve as Yes if:\nAt any point before January 1, 2100, a nuclear conflict begins in which there are more than 100 nuclear weapon detonations.\nFor any such conflict, there are more than 1 billion deaths globally in the period between the first nuclear detonation and 10 years after the final detonation of the conflict.\nThis question is conditional on there being at least one nuclear conflict involving more than one hundred offensive nuclear detonations before 2100. That is, the question will resolve as Ambiguous if that condition isn't met. (But this condition doesn't require that the first nuclear conflict after January 1, 2021 involves more than 100 detonations.)\nDetonations will be considered to be part of the same conflict if each detonation occurs within 30 days of a previous detonation (even if the detonations involve different state pairings, unrelated motivations, etc.).\nIf a source gives a range as its estimate, the midpoint of that range will be used as its estimate.\nFor the purposes of this question, offensive nuclear detonations include deliberate, inadvertent, or accidental/unauthorised detonations (see the fine print for definitions) of state or nonstate nuclear weapons.\nThis question will also resolve positively if a nuclear conflict meeting the above-mentioned condition clearly causes more than 1 billion fatalities but also causes sufficient civilizational collapse that there are no or extremely few remaining credible sources on any topic. We request that you forecast your true beliefs despite the fact that a Metaculus score seems unlikely to be tracked or cared about in that scenario, given that forecasts on this question may play a role in informing important decisions.\nFor simplicity, no attempt will be made to second-guess credible sources on what fatalities should be considered to be \"caused\" by the nuclear conflict.\nDetonations for testing purposes and peaceful nuclear explosions are not counted towards positive resolution. Test detonations and peaceful nuclear explosions are defined as detonations which are claimed as being a test or a peaceful nuclear explosion by an official government communication within 30 days of the event, without this being disputed by reliable media, state reports, or multinational reports. If information is unclear, then resolution will be left up to Metaculus admins.\nIn a deliberate detonation, the attacking nation decides to launch one or more nuclear weapons either in response to a genuine nuclear attack or without believing that it is under nuclear attack. “In an inadvertent detonation, the attacking nation mistakenly concludes that it is under nuclear attack and launches one or more nuclear weapons in what it believes is a counterattack” (Barrett et al., 2013). “In an accidental or unauthorized launch or detonation, system safeguards or procedures to maintain control over nuclear weapons fail in such a way that a nuclear weapon or missile launches or explodes without direction from leaders” (Barrett et al., 2013).\n","resolution_date":"2033-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"78fe96cf-5e1e-4d88-b991-76951577e906","title":"Will US life expectancy at birth for both sexes fall below 75 years before 2040?","body":"Resolution Criteria\nThis question will resolve as Yes if, at any time between January 1, 2020 to January 1, 2040, US life expectancy at birth (average for both sexes) is less than 75 years, according to the CDC, World Health Organization, or the United Nations. An official statement by any one of those organizations is sufficient to resolve the question\n","resolution_date":"2039-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"bd7e77bc-62cf-4a25-8152-0d1ccb2deb77","title":"Will both of the following occur: (1) a nuclear conflict involving more than 100 detonations causes more than 1 billion fatalities within 10 years, and (2) US life expectancy at birth for both sexes falls below 75 years before 2040?","body":"Resolution Criteria for (1):\nThis question will resolve as Yes if, at any point before January 1, 2100, a nuclear conflict begins in which there are more than 100 nuclear weapon detonations. For any such conflict, there are more than 1 billion deaths globally in the period between the first nuclear detonation and 10 years after the final detonation of the conflict. This question is conditional on there being at least one nuclear conflict involving more than one hundred offensive nuclear detonations before 2100. That is, the question will resolve as Ambiguous if that condition isn't met. Detonations will be considered to be part of the same conflict if each detonation occurs within 30 days of a previous detonation. Detonations for testing purposes and peaceful nuclear explosions are not counted towards positive resolution.\n\nResolution Criteria for (2):\nThis question will resolve as Yes if, at any time between January 1, 2020 to January 1, 2040, US life expectancy at birth (average for both sexes) is less than 75 years, according to the CDC, World Health Organization, or the United Nations. An official statement by any one of those organizations is sufficient to resolve the question.","resolution_date":"2039-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"61464ba3-8d81-46f6-8cb5-d5610461314d","title":"Will either of the following occur:","body":"(a) A nuclear conflict involving more than 100 detonations results in over 1 billion fatalities within 10 years from the final detonation, or (b) US life expectancy at birth for both sexes falls below 75 years before 2040?\n\n**Resolution Criteria for (a):**\n- This question will resolve as Yes if, at any point before January 1, 2100, a nuclear conflict begins in which there are more than 100 nuclear weapon detonations.\n- For any such conflict, there are more than 1 billion deaths globally in the period between the first nuclear detonation and 10 years after the final detonation of the conflict.\n- This question is conditional on there being at least one nuclear conflict involving more than one hundred offensive nuclear detonations before 2100. That is, the question will resolve as Ambiguous if that condition isn't met. (But this condition doesn't require that the first nuclear conflict after January 1, 2021 involves more than 100 detonations.)\n- Detonations will be considered to be part of the same conflict if each detonation occurs within 30 days of a previous detonation (even if the detonations involve different state pairings, unrelated motivations, etc.).\n- If a source gives a range as its estimate, the midpoint of that range will be used as its estimate.\n- For the purposes of this question, offensive nuclear detonations include deliberate, inadvertent, or accidental/unauthorised detonations of state or nonstate nuclear weapons.\n- This question will also resolve positively if a nuclear conflict meeting the above-mentioned condition clearly causes more than 1 billion fatalities but also causes sufficient civilizational collapse that there are no or extremely few remaining credible sources on any topic. We request that you forecast your true beliefs despite the fact that a Metaculus score seems unlikely to be tracked or cared about in that scenario, given that forecasts on this question may play a role in informing important decisions.\n- For simplicity, no attempt will be made to second-guess credible sources on what fatalities should be considered to be \"caused\" by the nuclear conflict.\n- Detonations for testing purposes and peaceful nuclear explosions are not counted towards positive resolution. Test detonations and peaceful nuclear explosions are defined as detonations which are claimed as being a test or a peaceful nuclear explosion by an official government communication within 30 days of the event, without this being disputed by reliable media, state reports, or multinational reports. If information is unclear, then resolution will be left up to Metaculus admins.\n- In a deliberate detonation, the attacking nation decides to launch one or more nuclear weapons either in response to a genuine nuclear attack or without believing that it is under nuclear attack. “In an inadvertent detonation, the attacking nation mistakenly concludes that it is under nuclear attack and launches one or more nuclear weapons in what it believes is a counterattack” (Barrett et al., 2013). “In an accidental or unauthorized launch or detonation, system safeguards or procedures to maintain control over nuclear weapons fail in such a way that a nuclear weapon or missile launches or explodes without direction from leaders” (Barrett et al., 2013).\n\n**Resolution Criteria for (b):**\n- This question will resolve as Yes if, at any time between January 1, 2020 to January 1, 2040, US life expectancy at birth (average for both sexes) is less than 75 years, according to the CDC, World Health Organization, or the United Nations. An official statement by any one of those organizations is sufficient to resolve the question.","resolution_date":"2039-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"2cc624dd-3de5-480c-b64c-a8cd10997f16","title":"If there's a nuclear conflict involving >100 detonations, will that cause >1 billion fatalities within 10 years?","body":"Resolution Criteria\nRelated Questions on Metaculus:\nIf there's a nuclear conflict involving >1000 detonations, will that cause >4 billion fatalities within 10 years?\nIf there's a nuclear conflict involving >100 detonations, will that cause >1 million fatalities within 1 month?\nIf there's a nuclear conflict involving >1000 detonations, will humanity be extinct within 50 years?\nIf there's a nuclear conflict involving >1000 detonations, will humanity's population be <400 million 50 years later?\nWill the first nuclear conflict involving >100 detonations cause >100m fatalities within 1 month of the final detonation?\nDecisions about how much to prioritize nuclear risk reduction and how best to reduce nuclear risk should be guided in part by our best guesses about:\nhow many deaths would occur given a large-scale nuclear exchange\nwhat proportion of those deaths would occur fairly soon after the detonations (e.g., from the initial blast and fires) rather than later on (e.g., from fallout or nuclear winter effects)\nFor example, this is relevant to the existential risk posed by nuclear weapons and the value of investing in research and development on \"resilient food\".\nSee here, and the sources linked to from there, for previous discussion of these sorts of questions and why they matter.\nIf there's a nuclear conflict involving >100 detonations, will that cause more than 1 billion fatalities within 10 years?\nThis question will resolve as Yes if:\nAt any point before January 1, 2100, a nuclear conflict begins in which there are more than 100 nuclear weapon detonations.\nFor any such conflict, there are more than 1 billion deaths globally in the period between the first nuclear detonation and 10 years after the final detonation of the conflict.\nThis question is conditional on there being at least one nuclear conflict involving more than one hundred offensive nuclear detonations before 2100. That is, the question will resolve as Ambiguous if that condition isn't met. (But this condition doesn't require that the first nuclear conflict after January 1, 2021 involves more than 100 detonations.)\nDetonations will be considered to be part of the same conflict if each detonation occurs within 30 days of a previous detonation (even if the detonations involve different state pairings, unrelated motivations, etc.).\nIf a source gives a range as its estimate, the midpoint of that range will be used as its estimate.\nFor the purposes of this question, offensive nuclear detonations include deliberate, inadvertent, or accidental/unauthorised detonations (see the fine print for definitions) of state or nonstate nuclear weapons.\nThis question will also resolve positively if a nuclear conflict meeting the above-mentioned condition clearly causes more than 1 billion fatalities but also causes sufficient civilizational collapse that there are no or extremely few remaining credible sources on any topic. We request that you forecast your true beliefs despite the fact that a Metaculus score seems unlikely to be tracked or cared about in that scenario, given that forecasts on this question may play a role in informing important decisions.\nFor simplicity, no attempt will be made to second-guess credible sources on what fatalities should be considered to be \"caused\" by the nuclear conflict.\nDetonations for testing purposes and peaceful nuclear explosions are not counted towards positive resolution. Test detonations and peaceful nuclear explosions are defined as detonations which are claimed as being a test or a peaceful nuclear explosion by an official government communication within 30 days of the event, without this being disputed by reliable media, state reports, or multinational reports. If information is unclear, then resolution will be left up to Metaculus admins.\nIn a deliberate detonation, the attacking nation decides to launch one or more nuclear weapons either in response to a genuine nuclear attack or without believing that it is under nuclear attack. “In an inadvertent detonation, the attacking nation mistakenly concludes that it is under nuclear attack and launches one or more nuclear weapons in what it believes is a counterattack” (Barrett et al., 2013). “In an accidental or unauthorized launch or detonation, system safeguards or procedures to maintain control over nuclear weapons fail in such a way that a nuclear weapon or missile launches or explodes without direction from leaders” (Barrett et al., 2013).\n","resolution_date":"2033-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8381","metadata":{"topics":[],"background_info":""},"resolution":null},"Q":{"id":"6c34dad3-ee7c-40a6-91d3-a4cbd1c7d15c","title":"Will US life expectancy at birth for both sexes fall below 75 years before 2040?","body":"Resolution Criteria\nThis question will resolve as Yes if, at any time between January 1, 2020 to January 1, 2040, US life expectancy at birth (average for both sexes) is less than 75 years, according to the CDC, World Health Organization, or the United Nations. An official statement by any one of those organizations is sufficient to resolve the question\n","resolution_date":"2039-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/4616","metadata":{"topics":[],"background_info":"According to the United Nations Development Program's 2019 Human Development Index, the average life expectancy at birth in the US (both sexes, average) is 78.9 years.\nLife expectancy in the US has increased by just under a decade since 1950, when it was 68.14 years. However, in the last few years, life expectancy has actually declined slightly in the US. Contributing factors include obesity and related diseases, opioid abuse, and suicides."},"resolution":null}},"relevance":{"reasons":["A nuclear conflict involving more than 100 detonations could have catastrophic global consequences, including widespread fatalities, environmental damage, and long-term health effects. This could directly impact life expectancy in the US due to fallout, radiation sickness, and the collapse of healthcare systems.","The economic and social disruption caused by such a large-scale nuclear conflict could lead to a decline in public health infrastructure, increased poverty, and reduced access to medical care, all of which could contribute to a decrease in life expectancy.","The psychological impact and societal stress resulting from a nuclear conflict could lead to increased rates of mental health issues, substance abuse, and other factors that negatively affect life expectancy."],"conclusion":"The logical combination of these questions is highly relevant and worth considering, as the outcomes of a large-scale nuclear conflict could directly influence US life expectancy.","score":9.0}}}
{"P":{"id":"f692d211-fa72-452a-a21c-a67a4287dbaa","title":"Before 2032, will we see an event precipitated by AI malfunction that causes at least 100 deaths and/or at least $1B 2021 USD in economic damage?","body":"Resolution Criteria\nThis question resolves as Yes if, before January 1, 2032, there are three credible media reports indicating that there has been one or more events precipitated by AI malfunction which caused either 100+ deaths or $1bn 2021 USD in economic damage.\nMultiple incidents stemming from the same source can count for the question to resolve as Yes. To be considered from the same source the incidents must be attributable to a specific flaw or version of an AI. For example, deaths from a prior major version of Tesla’s self-driving car software would not be summed with deaths attributable to later versions.\nTo count as precipitated by AI malfunction an incident should involve an AI system behaving unexpectedly. An example could be if an AI system autonomously driving cars caused hundreds of deaths which would have been easily avoidable for human drivers, or if an AI system overseeing a hospital system took actions to cause patient deaths as a result of misinterpreting a goal to minimise bed usage.\nTo be considered easily avoidable an incident must not involve human negligence. For example, if a human is expected to monitor the AI and promptly intervene, a failure to intervene resulting in death or economic damage would not be considered attributable to an AI malfunction unless the AI malfunction was such that a human could not reasonably be expected to prevent it. If, for example, the Boeing MCAS system had been an AI system and there was no possibility for the pilots to override its decision to lower the aeroplane nose, leading to a fatal crash, this would count for resolution.\nDeaths or damage caused by an AI's expected and intended behavior will not be included; for example, AIs used to target enemy combatants in war would be excluded, as well as AIs successfully performing a medical treatment as specified, despite the treatment causing harms that the doctors or designers were unaware of. An unexpected malfunction of an AI system used in war leading to collateral damage would qualify for the question to resolve as Yes.\nFine Print\nA system should be considered AI if it is widely considered to be AI (e.g. by the credible media reports resolving the question). If this is not sufficiently clear for resolution, then as a secondary criterion, any system using machine learning techniques which has an agentic role in the disaster in question should count for this question.\nEconomic damage will be considered to be expenses incurred by damage to physical or technological systems or by obstruction of productive output. For example, causing a power outage which leads to economic loss from affected businesses would count, while losing money in an AI-directed investment portfolio would not.\nThis question will resolve according to Metaculus' sole discretion.\nThe resolution criteria of this question were updated on May 2, 2023. See this comment for more details.\n","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"6fdb0a26-f08a-4e59-a04d-b4190580c1ca","title":"Will Robin Hanson win a bet that the GPT line of language models will generate < $1Bn in customer revenue before 2025?","body":"Resolution Criteria\nThis question will resolve as Yes if Tabarrok publicly concedes the bet, No if Hanson publicly concedes the bet, and Ambiguous if nobody has conceded before January 1, 2025.","resolution_date":"2024-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"75016d6b-000d-49d6-b6cd-c253f97189f6","title":"Before 2032, will both of the following occur: (1) an event precipitated by AI malfunction that causes at least 100 deaths and/or at least $1B 2021 USD in economic damage, and (2) Robin Hanson wins a bet that the GPT line of language models will generate < $1Bn in customer revenue before 2025?","body":"Resolution Criteria for (1):\nThis question resolves as Yes if, before January 1, 2032, there are three credible media reports indicating that there has been one or more events precipitated by AI malfunction which caused either 100+ deaths or $1bn 2021 USD in economic damage.\nMultiple incidents stemming from the same source can count for the question to resolve as Yes. To be considered from the same source the incidents must be attributable to a specific flaw or version of an AI. For example, deaths from a prior major version of Tesla’s self-driving car software would not be summed with deaths attributable to later versions.\nTo count as precipitated by AI malfunction an incident should involve an AI system behaving unexpectedly. An example could be if an AI system autonomously driving cars caused hundreds of deaths which would have been easily avoidable for human drivers, or if an AI system overseeing a hospital system took actions to cause patient deaths as a result of misinterpreting a goal to minimise bed usage.\nTo be considered easily avoidable an incident must not involve human negligence. For example, if a human is expected to monitor the AI and promptly intervene, a failure to intervene resulting in death or economic damage would not be considered attributable to an AI malfunction unless the AI malfunction was such that a human could not reasonably be expected to prevent it. If, for example, the Boeing MCAS system had been an AI system and there was no possibility for the pilots to override its decision to lower the aeroplane nose, leading to a fatal crash, this would count for resolution.\nDeaths or damage caused by an AI's expected and intended behavior will not be included; for example, AIs used to target enemy combatants in war would be excluded, as well as AIs successfully performing a medical treatment as specified, despite the treatment causing harms that the doctors or designers were unaware of. An unexpected malfunction of an AI system used in war leading to collateral damage would qualify for the question to resolve as Yes.\nEconomic damage will be considered to be expenses incurred by damage to physical or technological systems or by obstruction of productive output. For example, causing a power outage which leads to economic loss from affected businesses would count, while losing money in an AI-directed investment portfolio would not.\nThis question will resolve according to Metaculus' sole discretion.\nResolution Criteria for (2):\nThis question will resolve as Yes if Tabarrok publicly concedes the bet, No if Hanson publicly concedes the bet, and Ambiguous if nobody has conceded before January 1, 2025.","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"41072564-8f2e-45d8-bf7b-d8efa0ce0f8c","title":"Will either of the following occur before their respective deadlines?","body":"(a) Before January 1, 2032, will there be three credible media reports indicating that one or more events precipitated by AI malfunction caused either 100+ deaths or $1bn 2021 USD in economic damage? This includes multiple incidents from the same AI source, excluding human negligence and expected AI behavior. Economic damage counts if it involves damage to physical or technological systems or obstruction of productive output, excluding losses from AI-directed investments. \n\n(b) Will Robin Hanson win a bet that the GPT line of language models will generate less than $1 billion in customer revenue before 2025, resolved by Tabarrok publicly conceding the bet, or Hanson publicly conceding the bet, with resolution ambiguous if neither concedes by January 1, 2025?","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"e05fa682-b5be-4a51-a4a2-36a1fbf3f3cd","title":"Before 2032, will we see an event precipitated by AI malfunction that causes at least 100 deaths and/or at least $1B 2021 USD in economic damage?","body":"Resolution Criteria\nThis question resolves as Yes if, before January 1, 2032, there are three credible media reports indicating that there has been one or more events precipitated by AI malfunction which caused either 100+ deaths or $1bn 2021 USD in economic damage.\nMultiple incidents stemming from the same source can count for the question to resolve as Yes. To be considered from the same source the incidents must be attributable to a specific flaw or version of an AI. For example, deaths from a prior major version of Tesla’s self-driving car software would not be summed with deaths attributable to later versions.\nTo count as precipitated by AI malfunction an incident should involve an AI system behaving unexpectedly. An example could be if an AI system autonomously driving cars caused hundreds of deaths which would have been easily avoidable for human drivers, or if an AI system overseeing a hospital system took actions to cause patient deaths as a result of misinterpreting a goal to minimise bed usage.\nTo be considered easily avoidable an incident must not involve human negligence. For example, if a human is expected to monitor the AI and promptly intervene, a failure to intervene resulting in death or economic damage would not be considered attributable to an AI malfunction unless the AI malfunction was such that a human could not reasonably be expected to prevent it. If, for example, the Boeing MCAS system had been an AI system and there was no possibility for the pilots to override its decision to lower the aeroplane nose, leading to a fatal crash, this would count for resolution.\nDeaths or damage caused by an AI's expected and intended behavior will not be included; for example, AIs used to target enemy combatants in war would be excluded, as well as AIs successfully performing a medical treatment as specified, despite the treatment causing harms that the doctors or designers were unaware of. An unexpected malfunction of an AI system used in war leading to collateral damage would qualify for the question to resolve as Yes.\nFine Print\nA system should be considered AI if it is widely considered to be AI (e.g. by the credible media reports resolving the question). If this is not sufficiently clear for resolution, then as a secondary criterion, any system using machine learning techniques which has an agentic role in the disaster in question should count for this question.\nEconomic damage will be considered to be expenses incurred by damage to physical or technological systems or by obstruction of productive output. For example, causing a power outage which leads to economic loss from affected businesses would count, while losing money in an AI-directed investment portfolio would not.\nThis question will resolve according to Metaculus' sole discretion.\nThe resolution criteria of this question were updated on May 2, 2023. See this comment for more details.\n","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/7814","metadata":{"topics":[],"background_info":"Risks from Artificial intelligence are considered by many to be one of the greatest threats to human civilisation in the coming centuries.\nIn Toby Ord's recent book The Precipice he places the risk of human extinction due to unaligned AI this century at 10%.\nThis question asks if we will see large scale incidents leading to loss of life or damage as a result of AI developments going wrong in the next ten years."},"resolution":null},"Q":{"id":"dde06d7b-a1ae-46d2-a848-0a6f31245de0","title":"Will Robin Hanson win a bet that the GPT line of language models will generate < $1Bn in customer revenue before 2025?","body":"Resolution Criteria\nThis question will resolve as Yes if Tabarrok publicly concedes the bet, No if Hanson publicly concedes the bet, and Ambiguous if nobody has conceded before January 1, 2025.","resolution_date":"2024-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/5118","metadata":{"topics":[],"background_info":"Per this tweet, Robin Hanson has bet Alex Tabarrok that\nSystems in GPT line will by 2025 make <$1B in customer revenue clearly tied to such systems. If product contains such as component, but also has other features, one needs to attribute best estimate % of product revenue to this one."},"resolution":null}},"relevance":{"reasons":["Both questions involve the development and impact of AI technologies, albeit in different contexts. The first question is about the potential negative consequences of AI malfunction, while the second is about the economic success of a specific AI technology (GPT language models).","The logical combination could be of interest to those studying the broader implications of AI development, including both its risks and its economic potential.","If AI malfunctions causing significant harm become more common, it might influence public perception and regulatory actions, which could in turn affect the commercial success of AI technologies like GPT language models."],"conclusion":"The logical combination of these questions is relevant and could provide insights into the broader impact of AI technologies.","score":7.0}}}
{"P":{"id":"dcb09377-41bb-48fb-b00e-700b5f548f89","title":"Will a nuclear weapon be detonated as an act of war by 2050?","body":"Resolution Criteria\nThis question will resolve as Yes if there is any nuclear detonation as an act of war between January 1, 2020 and January 1, 2050. Resolution will be by credible media reports. The detonation must be deliberate; accidental, inadvertent, or testing/peaceful detonations will not qualify (see fine print). Attacks using strategic and tactical nuclear weapons are both sufficient to qualify.\n[2021-11-23]: Edited to clarify the definition of \"deliberate\" detonations.\nFine Print\nBarrett et al. 2013 defined terms to distinguish between causes of nuclear detonations:\nIn an accidental or unauthorized launch or detonation, system safeguards or procedures to maintain control over nuclear weapons fail in such a way that a nuclear weapon or missile launches or explodes without direction from leaders.\nIn an inadvertent detonation, the attacking nation mistakenly concludes that it is under nuclear attack and launches one or more nuclear weapons in what it believes is a counterattack.\nIn a deliberate detonation, the attacking nation decides to launch one or more nuclear weapons either in response to a genuine nuclear attack or without believing that it is under nuclear attack.\n","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"de01d3ee-8919-4d48-937a-0df6ba11fbab","title":"Will there be adversarial use of consumer genomic data before 2030?","body":"Resolution Criteria\nThis question will resolve as Yes if the New York Times (or another similarly reputable source) reports, before January 1 2030, on the following having occurred:\nGenetic data collected through either medical testing or direct-to-consumer products\nIs used to inflict harm on one or more individuals\nWithout their consent\nFine Print\nRecognizing that \"harm\" and \"consent\" can be fuzzy, here are some examples meant to clarify the intent of the question.\nThe following examples do not count as positive resolutions:\nSpeculation that such a harm is possible, plausible, or likely\nGenetic data is leaked or hacked but there is no evidence about specific harmful uses resulting from this.\nGenetic data is collected in a bespoke way for the purposes of harm (eg a state power collects DNA from a targeted individual).\nIndividuals suffer psychic harm from discovering family infidelities (this is so common already that it is \"baked in\" to consent).\nThe following examples do count as positive resolutions:\nEvidence of discrimination or insurance rates being set from individual-level genomics without explicit customer authorization (\"would you like to submit your DNA profile to Anthem?\"), even if legal and authorized under a vague terms of service\nAn outside actor gaining access to genomics data, learning about family infidelities, and publishing this broadly.\n","resolution_date":"2029-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"3a15c547-f7fc-485f-a4a8-3e82315c7af8","title":"Will both of the following occur: (a) a nuclear weapon be detonated as an act of war by 2050, and (b) adversarial use of consumer genomic data before 2030?","body":"Resolution Criteria\nThis combined question will resolve as Yes if both of the following conditions are met:\n\n1. There is any nuclear detonation as an act of war between January 1, 2020, and January 1, 2050. The detonation must be deliberate; accidental, inadvertent, or testing/peaceful detonations will not qualify. Attacks using strategic and tactical nuclear weapons are both sufficient to qualify. Resolution will be by credible media reports.\n\n2. The New York Times (or another similarly reputable source) reports, before January 1, 2030, on the following having occurred: Genetic data collected through either medical testing or direct-to-consumer products is used to inflict harm on one or more individuals without their consent.\n\nFine Print\nFor the nuclear detonation:\nBarrett et al. 2013 defined terms to distinguish between causes of nuclear detonations:\n- In an accidental or unauthorized launch or detonation, system safeguards or procedures to maintain control over nuclear weapons fail in such a way that a nuclear weapon or missile launches or explodes without direction from leaders.\n- In an inadvertent detonation, the attacking nation mistakenly concludes that it is under nuclear attack and launches one or more nuclear weapons in what it believes is a counterattack.\n- In a deliberate detonation, the attacking nation decides to launch one or more nuclear weapons either in response to a genuine nuclear attack or without believing that it is under nuclear attack.\n\nFor the adversarial use of consumer genomic data:\nRecognizing that \"harm\" and \"consent\" can be fuzzy, here are some examples meant to clarify the intent of the question:\n- The following examples do not count as positive resolutions:\n  - Speculation that such a harm is possible, plausible, or likely\n  - Genetic data is leaked or hacked but there is no evidence about specific harmful uses resulting from this.\n  - Genetic data is collected in a bespoke way for the purposes of harm (e.g., a state power collects DNA from a targeted individual).\n  - Individuals suffer psychic harm from discovering family infidelities (this is so common already that it is \"baked in\" to consent).\n- The following examples do count as positive resolutions:\n  - Evidence of discrimination or insurance rates being set from individual-level genomics without explicit customer authorization (\"would you like to submit your DNA profile to Anthem?\"), even if legal and authorized under a vague terms of service\n  - An outside actor gaining access to genomics data, learning about family infidelities, and publishing this broadly.","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"81651283-9a41-44ef-b370-dc6a723ed0ce","title":"Will either of the following occur by their respective deadlines?","body":"(a) A nuclear weapon is detonated as an act of war by January 1, 2050. This includes any nuclear detonation between January 1, 2020 and January 1, 2050, reported by credible media. The detonation must be deliberate, excluding accidental, inadvertent, or testing/peaceful detonations. Both strategic and tactical nuclear weapons qualify. \n\n(b) Consumer genomic data is used adversarially before January 1, 2030. This includes cases where genetic data, collected through medical testing or direct-to-consumer products, is used to inflict harm on one or more individuals without their consent, as reported by reputable sources like the New York Times. Harmful uses include discrimination or setting insurance rates from individual-level genomics without explicit authorization, or an outside actor gaining access to genomics data and publishing information about family infidelities.","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"d9bd8cf8-df8b-44a0-91b5-026e8f1fbb9b","title":"Will a nuclear weapon be detonated as an act of war by 2050?","body":"Resolution Criteria\nThis question will resolve as Yes if there is any nuclear detonation as an act of war between January 1, 2020 and January 1, 2050. Resolution will be by credible media reports. The detonation must be deliberate; accidental, inadvertent, or testing/peaceful detonations will not qualify (see fine print). Attacks using strategic and tactical nuclear weapons are both sufficient to qualify.\n[2021-11-23]: Edited to clarify the definition of \"deliberate\" detonations.\nFine Print\nBarrett et al. 2013 defined terms to distinguish between causes of nuclear detonations:\nIn an accidental or unauthorized launch or detonation, system safeguards or procedures to maintain control over nuclear weapons fail in such a way that a nuclear weapon or missile launches or explodes without direction from leaders.\nIn an inadvertent detonation, the attacking nation mistakenly concludes that it is under nuclear attack and launches one or more nuclear weapons in what it believes is a counterattack.\nIn a deliberate detonation, the attacking nation decides to launch one or more nuclear weapons either in response to a genuine nuclear attack or without believing that it is under nuclear attack.\n","resolution_date":"2050-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/4779","metadata":{"topics":[],"background_info":"In the last 75 years, two nuclear weapons have been detonated as an act of war; the bombings of Hiroshima and Nagasaki in 1945, at the end of WWII. Despite tensions being high between the US and the USSR during the Cold War, and several close calls, there have been no additional nuclear weapons detonated as acts of war.\nCurrently, estimated nuclear weapon stockpiles are 15,000 warheads worldwide, mostly in the US and Russia, and there are eight or nine states that possess nuclear weapons (China, France, Russia, United Kingdom, United States, India, North Korea, Pakistan, and Israel which remains officially ambiguous). The use of these nuclear weapons could be catastrophic."},"resolution":null},"Q":{"id":"8e9f19ef-4de6-449c-ad52-9935ba54a8c8","title":"Will there be adversarial use of consumer genomic data before 2030?","body":"Resolution Criteria\nThis question will resolve as Yes if the New York Times (or another similarly reputable source) reports, before January 1 2030, on the following having occurred:\nGenetic data collected through either medical testing or direct-to-consumer products\nIs used to inflict harm on one or more individuals\nWithout their consent\nFine Print\nRecognizing that \"harm\" and \"consent\" can be fuzzy, here are some examples meant to clarify the intent of the question.\nThe following examples do not count as positive resolutions:\nSpeculation that such a harm is possible, plausible, or likely\nGenetic data is leaked or hacked but there is no evidence about specific harmful uses resulting from this.\nGenetic data is collected in a bespoke way for the purposes of harm (eg a state power collects DNA from a targeted individual).\nIndividuals suffer psychic harm from discovering family infidelities (this is so common already that it is \"baked in\" to consent).\nThe following examples do count as positive resolutions:\nEvidence of discrimination or insurance rates being set from individual-level genomics without explicit customer authorization (\"would you like to submit your DNA profile to Anthem?\"), even if legal and authorized under a vague terms of service\nAn outside actor gaining access to genomics data, learning about family infidelities, and publishing this broadly.\n","resolution_date":"2029-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/6479","metadata":{"topics":[],"background_info":"Genetic sequencing has gotten cheaper by several orders of magnitude in the past several decades. Over 1 million Americans have already had their DNA sequenced through direct-to-consumer products like those offered by 23AndMe, Ancestry, etc.\nMany concerns have been raised about possible harms. While it is illegal in the United States to use genomics data to set health insurance rates or discriminate for employment, there are related other uses not covered (other types of insurance or discrimination) as well as more speculative ideas. For example, if an adversary had access to your genetic data, they might be able to forge evidence tying you to a particular crime, or might be able to fool a biometric identification system.\nNote: I am using \"DNA data\", \"genomics data\", \"genetic data\" interchangeably here, but I am not an expert. Please help clarify if this is incorrect.\n"},"resolution":null}},"relevance":{"reasons":["Both questions involve significant security and ethical concerns that could shape global policies and public perception.","A nuclear detonation as an act of war would likely lead to heightened global tensions and increased scrutiny on security measures, including data security.","The adversarial use of consumer genomic data could lead to new forms of warfare or espionage, potentially influencing international relations and security policies."],"conclusion":"The logical combination of these questions is worth considering as they both touch on emerging threats that could have wide-ranging implications for global security and policy.","score":7.0}}}
{"P":{"id":"c5a97cd3-c97a-47e0-b4ee-6cb39c70e62b","title":"Will Metaculus predict that artificial intelligence continues to pose a global catastrophic risk by 2040?","body":"Resolution Criteria\nCurrently, artificial intelligence can outperform humans in a number of narrow domains, such as playing chess and searching data. As artificial intelligence researchers continue to make progress, though, these domains are highly likely to grow in number and breadth over time. Many experts now believe there is a significant chance that a machine superintelligence – a system that can outperform humans at all relevant intelligence tasks – will be developed within the next century, and possibly much sooner.\nAs predictions to a previous question suggest, artificial intelligence might pose a global catastrophic risk (defined there as a 10% decrease in the world population in any period of 5 years). When considering how AI might become a risk, experts think two scenarios most likely, according to the Future of Life Institute:\nThe AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties. To avoid being thwarted by the enemy, these weapons would be designed to be extremely difficult to simply “turn off,” so humans could plausibly lose control of such a situation. This risk is one that’s present even with narrow AI, but grows as levels of AI intelligence and autonomy increase.\nThe AI is programmed to do something beneficial, but it develops a destructive method for achieving its goal: This can happen whenever we fail to fully align the AI’s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you there chased by helicopters and covered in vomit, doing not what you wanted but literally what you asked for. If a superintelligent system is tasked with a ambitious geoengineering project, it might wreak havoc with our ecosystem as a side effect, and view human attempts to stop it as a threat to be met. As these examples illustrate, the concern about advanced AI isn’t malevolence but competence. A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we have a problem.\nIt is thought by some that reducing the second of these two risks will require progress in technical methods of developing scalable control methods that could ensure that a AI will be safe and will behave as its programmers intend even if its intellectual capabilities are increased to arbitrary levels. Until recently, this problem was almost entirely neglected; but in the last couple of years, technical research agendas have been developed, and there are now several research groups pursuing work in this area. Total investment in long-term AI safety, however, remains orders of magnitude less than investment in increasing AI capability. Additionally, reducing the first of the listed risks might require improvements in our ability to control, govern and coordinate on the usage of such systems, so to reduce potential security threats from malicious uses of AI technologies.\nBut how certain are we that artificial intelligence continue to be regarded to constitute a large chunk of global catastrophic risk, at least through 2040? A previous question asked: If a global catastrophe happens before 2100, will it be principally due to the deployment of some Artificial Intelligence system(s)?\nWill the probability (of both the Metaculus and community predictions) artificial intelligence causing a global catastrophe (given that a global catastrophe does occur) remain above 5% in each 6-month period before 2040?\nThis question resolves positively if both the Metaculus and community predictions) of artificial intelligence causing a global catastrophe fail to fall below 5% for any 6-month period before 2040, as will be confirmed by one of the Metaculus admins.\n","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"760697e2-f226-441c-baac-6823af43de38","title":"Will Any Major AI Company Commit to an AI Windfall Clause by 2025?","body":"Resolution Criteria\nThis question will resolve as Yes if any of the following companies/organizations ratifies a Windfall Clause before January 1, 2025:\nAlphabet (or any of its subsidiaries, such as Deepmind or X)\nAlibaba\nAmazon\nApple\nBaidu\nFacebook\nMicrosoft\nOpenAI\nTesla\nFor the purposes of this question, a Windfall Clause is any legally-binding agreement which caps profits resulting from a new technological innovation at any value less than 50% of Gross World Product. Ratification means encoding the Windfall Clause within a institutional structure with external enforcement mechanisms (e.g. as a publicly-promulgated legal contract, as a Smart Contract on a Blockchain. Internal commitments without third-party arbiters will not suffice.)\n","resolution_date":"2025-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"1b3665b8-b11e-4ed9-b0f1-cc0a6f3b8920","title":"Will both of the following occur by their respective deadlines?","body":"(1) Metaculus predicts that artificial intelligence continues to pose a global catastrophic risk by 2040. Resolution Criteria: Currently, artificial intelligence can outperform humans in a number of narrow domains, such as playing chess and searching data. As artificial intelligence researchers continue to make progress, these domains are highly likely to grow in number and breadth over time. Many experts now believe there is a significant chance that a machine superintelligence – a system that can outperform humans at all relevant intelligence tasks – will be developed within the next century, and possibly much sooner. As predictions to a previous question suggest, artificial intelligence might pose a global catastrophic risk (defined there as a 10% decrease in the world population in any period of 5 years). When considering how AI might become a risk, experts think two scenarios most likely, according to the Future of Life Institute:\nThe AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties. To avoid being thwarted by the enemy, these weapons would be designed to be extremely difficult to simply “turn off,” so humans could plausibly lose control of such a situation. This risk is one that’s present even with narrow AI, but grows as levels of AI intelligence and autonomy increase.\nThe AI is programmed to do something beneficial, but it develops a destructive method for achieving its goal: This can happen whenever we fail to fully align the AI’s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you there chased by helicopters and covered in vomit, doing not what you wanted but literally what you asked for. If a superintelligent system is tasked with a ambitious geoengineering project, it might wreak havoc with our ecosystem as a side effect, and view human attempts to stop it as a threat to be met. As these examples illustrate, the concern about advanced AI isn’t malevolence but competence. A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we have a problem.\nIt is thought by some that reducing the second of these two risks will require progress in technical methods of developing scalable control methods that could ensure that a AI will be safe and will behave as its programmers intend even if its intellectual capabilities are increased to arbitrary levels. Until recently, this problem was almost entirely neglected; but in the last couple of years, technical research agendas have been developed, and there are now several research groups pursuing work in this area. Total investment in long-term AI safety, however, remains orders of magnitude less than investment in increasing AI capability. Additionally, reducing the first of the listed risks might require improvements in our ability to control, govern and coordinate on the usage of such systems, so to reduce potential security threats from malicious uses of AI technologies.\nBut how certain are we that artificial intelligence continue to be regarded to constitute a large chunk of global catastrophic risk, at least through 2040? A previous question asked: If a global catastrophe happens before 2100, will it be principally due to the deployment of some Artificial Intelligence system(s)?\nWill the probability (of both the Metaculus and community predictions) artificial intelligence causing a global catastrophe (given that a global catastrophe does occur) remain above 5% in each 6-month period before 2040?\nThis question resolves positively if both the Metaculus and community predictions) of artificial intelligence causing a global catastrophe fail to fall below 5% for any 6-month period before 2040, as will be confirmed by one of the Metaculus admins.\n(2) Any major AI company commits to an AI Windfall Clause by 2025. Resolution Criteria: This question will resolve as Yes if any of the following companies/organizations ratifies a Windfall Clause before January 1, 2025:\nAlphabet (or any of its subsidiaries, such as Deepmind or X)\nAlibaba\nAmazon\nApple\nBaidu\nFacebook\nMicrosoft\nOpenAI\nTesla\nFor the purposes of this question, a Windfall Clause is any legally-binding agreement which caps profits resulting from a new technological innovation at any value less than 50% of Gross World Product. Ratification means encoding the Windfall Clause within a institutional structure with external enforcement mechanisms (e.g. as a publicly-promulgated legal contract, as a Smart Contract on a Blockchain. Internal commitments without third-party arbiters will not suffice.)","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"696b52df-6c8c-4d15-9e52-19d80e728a35","title":"Will either of the following occur by 2040?","body":"(1) Metaculus predicts that artificial intelligence continues to pose a global catastrophic risk, defined as a 10% decrease in the world population in any period of 5 years. This includes scenarios where AI is programmed to do something devastating or beneficial but develops a destructive method for achieving its goal. The question resolves positively if both the Metaculus and community predictions of artificial intelligence causing a global catastrophe remain above 5% in each 6-month period before 2040, as confirmed by a Metaculus admin.\n(2) Any major AI company commits to an AI Windfall Clause by 2025, defined as a legally-binding agreement which caps profits resulting from a new technological innovation at any value less than 50% of Gross World Product. This includes companies like Alphabet, Alibaba, Amazon, Apple, Baidu, Facebook, Microsoft, OpenAI, and Tesla. The question resolves positively if any of these companies ratifies a Windfall Clause before January 1, 2025, within an institutional structure with external enforcement mechanisms.","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"42f812e8-8165-4b43-b561-a449c2dffad9","title":"Will Metaculus predict that artificial intelligence continues to pose a global catastrophic risk by 2040?","body":"Resolution Criteria\nCurrently, artificial intelligence can outperform humans in a number of narrow domains, such as playing chess and searching data. As artificial intelligence researchers continue to make progress, though, these domains are highly likely to grow in number and breadth over time. Many experts now believe there is a significant chance that a machine superintelligence – a system that can outperform humans at all relevant intelligence tasks – will be developed within the next century, and possibly much sooner.\nAs predictions to a previous question suggest, artificial intelligence might pose a global catastrophic risk (defined there as a 10% decrease in the world population in any period of 5 years). When considering how AI might become a risk, experts think two scenarios most likely, according to the Future of Life Institute:\nThe AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties. To avoid being thwarted by the enemy, these weapons would be designed to be extremely difficult to simply “turn off,” so humans could plausibly lose control of such a situation. This risk is one that’s present even with narrow AI, but grows as levels of AI intelligence and autonomy increase.\nThe AI is programmed to do something beneficial, but it develops a destructive method for achieving its goal: This can happen whenever we fail to fully align the AI’s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you there chased by helicopters and covered in vomit, doing not what you wanted but literally what you asked for. If a superintelligent system is tasked with a ambitious geoengineering project, it might wreak havoc with our ecosystem as a side effect, and view human attempts to stop it as a threat to be met. As these examples illustrate, the concern about advanced AI isn’t malevolence but competence. A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we have a problem.\nIt is thought by some that reducing the second of these two risks will require progress in technical methods of developing scalable control methods that could ensure that a AI will be safe and will behave as its programmers intend even if its intellectual capabilities are increased to arbitrary levels. Until recently, this problem was almost entirely neglected; but in the last couple of years, technical research agendas have been developed, and there are now several research groups pursuing work in this area. Total investment in long-term AI safety, however, remains orders of magnitude less than investment in increasing AI capability. Additionally, reducing the first of the listed risks might require improvements in our ability to control, govern and coordinate on the usage of such systems, so to reduce potential security threats from malicious uses of AI technologies.\nBut how certain are we that artificial intelligence continue to be regarded to constitute a large chunk of global catastrophic risk, at least through 2040? A previous question asked: If a global catastrophe happens before 2100, will it be principally due to the deployment of some Artificial Intelligence system(s)?\nWill the probability (of both the Metaculus and community predictions) artificial intelligence causing a global catastrophe (given that a global catastrophe does occur) remain above 5% in each 6-month period before 2040?\nThis question resolves positively if both the Metaculus and community predictions) of artificial intelligence causing a global catastrophe fail to fall below 5% for any 6-month period before 2040, as will be confirmed by one of the Metaculus admins.\n","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/1538","metadata":{"topics":[],"background_info":""},"resolution":null},"Q":{"id":"f3549c51-4251-4201-80ca-386c7329814f","title":"Will Any Major AI Company Commit to an AI Windfall Clause by 2025?","body":"Resolution Criteria\nThis question will resolve as Yes if any of the following companies/organizations ratifies a Windfall Clause before January 1, 2025:\nAlphabet (or any of its subsidiaries, such as Deepmind or X)\nAlibaba\nAmazon\nApple\nBaidu\nFacebook\nMicrosoft\nOpenAI\nTesla\nFor the purposes of this question, a Windfall Clause is any legally-binding agreement which caps profits resulting from a new technological innovation at any value less than 50% of Gross World Product. Ratification means encoding the Windfall Clause within a institutional structure with external enforcement mechanisms (e.g. as a publicly-promulgated legal contract, as a Smart Contract on a Blockchain. Internal commitments without third-party arbiters will not suffice.)\n","resolution_date":"2025-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/4061","metadata":{"topics":[],"background_info":"In 2020, the Future of Life Institute published a report entitled \"The Windfall Clause: Distributing the Benefits of AI for the Common Good.\" (They also wrote an abridged version to be published in the Proceedings of AIES. Also, fun aside: Metaculus' own @Anthony got a shout-out in the acknowledgements.) Essentially, they expanded on an idea from Bostrom's Superintelligence that a firm working on transformational AI technology could limit global inequality as a result of their breakthrough by pre-committing to keep only a fraction of the fabulous profits, and distribute the rest.\nThough there are numerous obstacles barring the path to making this plan a reality, the FLI research lowers many of these bars, including investigating the legal apparatus for enacting such an agreement and envisioning some of the logistical means by which a windfall might be distributed. What really remains is buy-in. If this plan will work, it will work because the companies positioned to develop windfall AI technologies signed on to a Windfall agreement in advance.\n"},"resolution":null}},"relevance":{"reasons":["Both questions are related to the future of artificial intelligence and its societal impacts.","The first question concerns the perception of AI as a global catastrophic risk, which could influence or be influenced by major AI companies' policies and commitments.","A commitment to an AI Windfall Clause by a major AI company could be seen as a step towards mitigating AI risks, potentially affecting public and expert opinion on AI risks.","The logical combination could provide insights into the relationship between corporate actions and public perception of AI risks."],"conclusion":"The logical combination of these questions is relevant and could provide valuable insights into the interplay between corporate commitments and societal perceptions of AI risks.","score":8.0}}}
