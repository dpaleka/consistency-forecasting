{"P":{"id":"3de6dacd-47cc-453b-b4fe-0e445c1a9b8d","title":"Will Saudi Arabia normalize relations with Israel by 2031 if Iran does not get a nuclear bomb by then?","body":"Resolution Criteria\nIf the Iran nuke question resolves negatively (i.e. the Iranian Regime credibly states it has a nuclear weapon or a weapon test, as judged by media reports), then this question will resolve positively if at least five reliable media outlets report that the State of Israel is legally recognized by the Kingdom of Saudi Arabia before 2031-01-01, and negatively otherwise. Recognition is generally considered valid if it is declared by law, or declared through a large international body such as the United Nations. If the Iran nuke question resolves positively or ambiguously, this question resolves ambiguously\n","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"ce3dc14d-0091-4787-85c1-7ded32180c8e","title":"Will Republicans win control of the US House of Representatives in 2024?","body":"Resolution Criteria\nResolves true if Republicans^ win 50% + 1 or more seats in the US House. Resolves false if Democrats^ win 50% + 1 or more seats. If neither of those parties wins 50% + 1 or more (e.g. an unaligned 3rd party wins seats), then the question resolves ambiguously.\nFine Print\n.* If the total number of seats in the US House changes, then the number needed for a majority changes with it.\n.^ Independents who caucus with the Republicans will be counted as Republicans; likewise independents who caucus with Democrats.\n","resolution_date":"2024-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"9ef929e7-23f1-43f4-b738-b70b52d78585","title":"Will both of the following occur: (1) Saudi Arabia normalizes relations with Israel by 2031 if Iran does not get a nuclear bomb by then, and (2) Republicans win control of the US House of Representatives in 2024?","body":"Resolution Criteria for (1):\nIf the Iran nuke question resolves negatively (i.e., the Iranian Regime credibly states it has a nuclear weapon or a weapon test, as judged by media reports), then this question will resolve positively if at least five reliable media outlets report that the State of Israel is legally recognized by the Kingdom of Saudi Arabia before 2031-01-01, and negatively otherwise. Recognition is generally considered valid if it is declared by law, or declared through a large international body such as the United Nations. If the Iran nuke question resolves positively or ambiguously, this question resolves ambiguously.\n\nResolution Criteria for (2):\nResolves true if Republicans win 50% + 1 or more seats in the US House. Resolves false if Democrats win 50% + 1 or more seats. If neither of those parties wins 50% + 1 or more (e.g., an unaligned 3rd party wins seats), then the question resolves ambiguously.\n\nFine Print for (2):\nIf the total number of seats in the US House changes, then the number needed for a majority changes with it. Independents who caucus with the Republicans will be counted as Republicans; likewise, independents who caucus with Democrats.","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"ccee6af0-8b4d-409d-b91b-544819269de4","title":"Will either of the following occur: (1) Saudi Arabia normalizes relations with Israel by 2031 if Iran does not get a nuclear bomb by then, or (2) Republicans win control of the US House of Representatives in 2024?","body":"Resolution Criteria for (1):\nIf the Iran nuke question resolves negatively (i.e., the Iranian Regime credibly states it has a nuclear weapon or a weapon test, as judged by media reports), then this question will resolve positively if at least five reliable media outlets report that the State of Israel is legally recognized by the Kingdom of Saudi Arabia before 2031-01-01, and negatively otherwise. Recognition is generally considered valid if it is declared by law, or declared through a large international body such as the United Nations. If the Iran nuke question resolves positively or ambiguously, this question resolves ambiguously.\n\nResolution Criteria for (2):\nResolves true if Republicans win 50% + 1 or more seats in the US House. Resolves false if Democrats win 50% + 1 or more seats. If neither of those parties wins 50% + 1 or more (e.g., an unaligned 3rd party wins seats), then the question resolves ambiguously.\n\nFine Print for (2):\nIf the total number of seats in the US House changes, then the number needed for a majority changes with it. Independents who caucus with the Republicans will be counted as Republicans; likewise, independents who caucus with Democrats.","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"495de526-2f79-4c19-a4c8-c7c53095df2c","title":"Will Saudi Arabia normalize relations with Israel by 2031 if Iran does not get a nuclear bomb by then?","body":"Resolution Criteria\nIf the Iran nuke question resolves negatively (i.e. the Iranian Regime credibly states it has a nuclear weapon or a weapon test, as judged by media reports), then this question will resolve positively if at least five reliable media outlets report that the State of Israel is legally recognized by the Kingdom of Saudi Arabia before 2031-01-01, and negatively otherwise. Recognition is generally considered valid if it is declared by law, or declared through a large international body such as the United Nations. If the Iran nuke question resolves positively or ambiguously, this question resolves ambiguously\n","resolution_date":"2031-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8109","metadata":{"topics":[],"background_info":"In 2020, Israel normalized relations with the United Arab Emirates, Bahrain, and Morocco in the Abraham Accords. This comes in the face of increasing cooperation between the United States, Israel, and the Gulf countries who all face a threat from Iran and have expressed concern about Iran's nuclear program. A previous question asked if Saudi Arabia would normalize relations with Israel in 2021."},"resolution":null},"Q":{"id":"2401eeb8-9263-4680-aa97-eac43a5998f7","title":"Will Republicans win control of the US House of Representatives in 2024?","body":"Resolution Criteria\nResolves true if Republicans^ win 50% + 1 or more seats in the US House. Resolves false if Democrats^ win 50% + 1 or more seats. If neither of those parties wins 50% + 1 or more (e.g. an unaligned 3rd party wins seats), then the question resolves ambiguously.\nFine Print\n.* If the total number of seats in the US House changes, then the number needed for a majority changes with it.\n.^ Independents who caucus with the Republicans will be counted as Republicans; likewise independents who caucus with Democrats.\n","resolution_date":"2024-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/7849","metadata":{"topics":[],"background_info":"In 2020, Republicans gained seats in the House of Representatives despite losing the White House, leaving the Democrats with the thinnest margin in decades. Historically, there is often a backlash against the winner of the presidential election during the first term, so majority control could flip. However, redistricting following the 2020 Census will have its effect too.\nIf Republicans win 218* seats or more in 2024, they will secure a majority in the house. Will that happen?\n435* seats will be contested in 2024 (most in November, but some runoffs are possible). The question resolves after enough races have been called by reliable media reports to give a majority to one party or another. In case of ongoing ambiguity, the question can resolve when the Congress convenes in January 2025 and certifies its membership.\n"},"resolution":null}},"relevance":{"reasons":["Geopolitical Dynamics: The normalization of relations between Saudi Arabia and Israel could be influenced by the broader geopolitical landscape, including the stance of the US government. A Republican-controlled House might push for policies that encourage or discourage such normalization.","US Foreign Policy: Republicans winning control of the House could lead to changes in US foreign policy towards the Middle East, which might impact Saudi-Israel relations.","Legislative Influence: A Republican House might pass legislation that affects US-Saudi or US-Israel relations, indirectly influencing the likelihood of normalization."],"conclusion":"There are plausible reasons to consider the logical combination of these questions, as US political dynamics can influence international relations.","score":7.0}}}
{"P":{"id":"0adde9f2-840b-4a6b-9af5-d28586d2f8f9","title":"If there is at least one nuclear detonation by 2030, in which countries will there be at least one fatality? (China)","body":"Resolution Criteria\nThis group is conditional on at least 1 offensive nuclear detonation occuring at any time between January 1, 2021 and January 1, 2030. If no such detonation occurs anywhere on Earth in this period, all sub-questions below will resolve as Ambiguous.\nIf there is at least one offensive nuclear detonation in this time period, the following sub-questions will resolve as Yes if there is at least 1 fatality from any nuclear detonation occuring in that country (or regions), at any point between January 1, 2021 to January 1, 2030. If there is no such fatality, the sub-question will resolve as No. If the only nuclear detonations in this period occur outside of all of the following areas (for example, they occur in international waters, or a region(s) not listed below), all options below will resolve as No.\nThe fatality must be caused by the immediate effects of the detonation; fatalities caused by eg. fallout, rioting, or climate effects will be excluded from resolution.\nFor this question, an \"offensive nuclear detonation\" includes deliberate, inadvertent, or accidental/unauthorised nuclear detonations of by state or nonstate actors, but doesn't include detonations for testing purposes or peaceful nuclear explosions (even if such detonations cause substantial damage).\nFine Print\nTest detonations and peaceful nuclear explosions are defined as detonations which are claimed as being a test or a peaceful nuclear explosion by an official government communication within 30 days of the event, without this being disputed by reliable media, state reports, or multinational reports. Final determination of whether a detonation is a test/peaceful detonation is at the sole discretion of Metaculus Admins.\n\nA detonation will be considered \"within\" a given country if it occurs less than 20 kilometers above the land or sea level in that country's land territory, internal waters, or within 10 kilometers of their coastline.\n","resolution_date":"2030-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"f2610154-4c80-469b-8067-e7c9cde9e345","title":"Will humans go extinct before 2100?","body":"Resolution Criteria\nThis question will resolve as Yes if there are no known humans alive on January 1, 2100. For these purposes we'll define humans as biological creatures who have as their ancestors – via a chain of live births from mothers – circa 2000 humans OR who could mate with circa 2000 humans to produce viable offspring. (So AIs, ems, genetically engineered beings of a different species brought up in artificial wombs, etc. would not count). Any living humans living anywhere in the observable universe (or multiverse) (who are known to the entities operating Metaculus) on that date will be sufficient to resolve the question negatively.\nN.B. Even though it is obviously the case that if human extinction occurs Metaculus points won't be very valuable anymore and that it will be practically impossible to check for true human extinction (zero humans left), I would like to ask people not to let this fact influence their prediction and to predict in good faith\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"148e0fad-5855-40a6-b3e5-fac71c5ac58b","title":"Will both of the following occur: (1) there is at least one offensive nuclear detonation between January 1, 2021 and January 1, 2030, resulting in at least one fatality in China, and (2) humans go extinct before 2100?","body":"Resolution Criteria for (1):\nThis part of the question is conditional on at least 1 offensive nuclear detonation occurring at any time between January 1, 2021 and January 1, 2030. If no such detonation occurs anywhere on Earth in this period, this part will resolve as Ambiguous.\nIf there is at least one offensive nuclear detonation in this time period, this part will resolve as Yes if there is at least 1 fatality from any nuclear detonation occurring in China, at any point between January 1, 2021 to January 1, 2030. The fatality must be caused by the immediate effects of the detonation; fatalities caused by e.g., fallout, rioting, or climate effects will be excluded from resolution. An \"offensive nuclear detonation\" includes deliberate, inadvertent, or accidental/unauthorised nuclear detonations by state or nonstate actors, but doesn't include detonations for testing purposes or peaceful nuclear explosions.\n\nResolution Criteria for (2):\nThis part will resolve as Yes if there are no known humans alive on January 1, 2100. For these purposes, humans are defined as biological creatures who have as their ancestors – via a chain of live births from mothers – circa 2000 humans OR who could mate with circa 2000 humans to produce viable offspring. Any living humans anywhere in the observable universe (or multiverse) (who are known to the entities operating Metaculus) on that date will be sufficient to resolve the question negatively.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"31f06f6c-0309-4f8a-ac69-2decad9effe6","title":"Will either of the following occur:","body":"(1) There is at least one fatality in China due to an offensive nuclear detonation occurring between January 1, 2021, and January 1, 2030. This is conditional on at least one offensive nuclear detonation happening anywhere on Earth during this period. An offensive nuclear detonation includes deliberate, inadvertent, or accidental/unauthorized detonations by state or nonstate actors, excluding detonations for testing purposes or peaceful nuclear explosions. A detonation will be considered \"within\" China if it occurs less than 20 kilometers above the land or sea level in China's land territory, internal waters, or within 10 kilometers of their coastline. The fatality must be caused by the immediate effects of the detonation.\n(2) Humans go extinct before 2100, defined as there being no known humans alive on January 1, 2100, anywhere in the observable universe or multiverse. Humans are defined as biological creatures who have as their ancestors – via a chain of live births from mothers – circa 2000 humans or who could mate with circa 2000 humans to produce viable offspring.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"5a591ff1-c917-4e22-bb61-c91fc8f79d9d","title":"If there is at least one nuclear detonation by 2030, in which countries will there be at least one fatality? (China)","body":"Resolution Criteria\nThis group is conditional on at least 1 offensive nuclear detonation occuring at any time between January 1, 2021 and January 1, 2030. If no such detonation occurs anywhere on Earth in this period, all sub-questions below will resolve as Ambiguous.\nIf there is at least one offensive nuclear detonation in this time period, the following sub-questions will resolve as Yes if there is at least 1 fatality from any nuclear detonation occuring in that country (or regions), at any point between January 1, 2021 to January 1, 2030. If there is no such fatality, the sub-question will resolve as No. If the only nuclear detonations in this period occur outside of all of the following areas (for example, they occur in international waters, or a region(s) not listed below), all options below will resolve as No.\nThe fatality must be caused by the immediate effects of the detonation; fatalities caused by eg. fallout, rioting, or climate effects will be excluded from resolution.\nFor this question, an \"offensive nuclear detonation\" includes deliberate, inadvertent, or accidental/unauthorised nuclear detonations of by state or nonstate actors, but doesn't include detonations for testing purposes or peaceful nuclear explosions (even if such detonations cause substantial damage).\nFine Print\nTest detonations and peaceful nuclear explosions are defined as detonations which are claimed as being a test or a peaceful nuclear explosion by an official government communication within 30 days of the event, without this being disputed by reliable media, state reports, or multinational reports. Final determination of whether a detonation is a test/peaceful detonation is at the sole discretion of Metaculus Admins.\n\nA detonation will be considered \"within\" a given country if it occurs less than 20 kilometers above the land or sea level in that country's land territory, internal waters, or within 10 kilometers of their coastline.\n","resolution_date":"2030-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/8364","metadata":{"topics":[],"background_info":"Which countries would be targeted in a nuclear conflict is relevant both for understanding the total risk posed by nuclear weapons and for understanding how to reduce that risk."},"resolution":null},"Q":{"id":"3c92f48c-dce3-4018-8bf8-bbc1066e9f3e","title":"Will humans go extinct before 2100?","body":"Resolution Criteria\nThis question will resolve as Yes if there are no known humans alive on January 1, 2100. For these purposes we'll define humans as biological creatures who have as their ancestors – via a chain of live births from mothers – circa 2000 humans OR who could mate with circa 2000 humans to produce viable offspring. (So AIs, ems, genetically engineered beings of a different species brought up in artificial wombs, etc. would not count). Any living humans living anywhere in the observable universe (or multiverse) (who are known to the entities operating Metaculus) on that date will be sufficient to resolve the question negatively.\nN.B. Even though it is obviously the case that if human extinction occurs Metaculus points won't be very valuable anymore and that it will be practically impossible to check for true human extinction (zero humans left), I would like to ask people not to let this fact influence their prediction and to predict in good faith\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/578","metadata":{"topics":[],"background_info":"While the general feeling of most people, especially now that the cold war is (mostly) over, is that the risk of human extinction is extremely small, experts have assigned a significantly higher probability to the event.\nIn 2008 an informal poll at the Global Catastrophic Risk Conference at the University of Oxford yielded a median probability of human extinction by 2100 of 19%. Yet, one might want to be cautious when using this result as a good estimate of the true probability of human extinction, as there may be a powerful selection effect at play. Only those who assign a high probability to human extinction are likely to attend the Global Catastrophic Risk Conference in the first place, meaning that the survey was effectively sampling opinions from one extreme tail of the opinion distribution on the subject. Indeed, the conference report itself stated that the findings should be taken 'with a grain of salt'."},"resolution":null}},"relevance":{"reasons":["A nuclear detonation causing fatalities in China could be part of a larger global conflict that might increase the probability of human extinction before 2100.","The occurrence of a nuclear detonation might indicate geopolitical instability, which could be a factor in scenarios leading to human extinction.","The use of nuclear weapons could have long-term environmental and health impacts that might contribute to existential risks for humanity."],"conclusion":"The logical combination of these questions is somewhat relevant because a nuclear detonation could be a factor in scenarios that increase the risk of human extinction.","score":6.0}}}
{"P":{"id":"628fbd20-7c84-4bb9-bea2-88f555b9c62b","title":"Will an oracle superintelligence be developed before a general superintelligence?","body":"Resolution Criteria\nThis question will immediately resolve positively if an AI limited to answering questions achieves reliably superhuman performance across virtually all questions of interest before an AI achieves generally superhuman performance across virtually all human activities of interest. If a generally superhuman AI is developed first, the question will immediately resolve negatively. By default, the question will resolve ambiguously on the resolve date, 2099-12-31.\nSuccessful creation of either technology would presumably be extremely obvious and uncontroversial, with a great amount of media coverage and scientific attention. However, if there is significant disagreement over whether a given apparent achievement resolves the question, it will be determined by Metaculus moderators\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"Q":{"id":"f72d3c0c-8755-4d39-a56d-55f2dd2ebd1e","title":"Will Metaculus predict that artificial intelligence continues to pose a global catastrophic risk by 2040?","body":"Resolution Criteria\nCurrently, artificial intelligence can outperform humans in a number of narrow domains, such as playing chess and searching data. As artificial intelligence researchers continue to make progress, though, these domains are highly likely to grow in number and breadth over time. Many experts now believe there is a significant chance that a machine superintelligence – a system that can outperform humans at all relevant intelligence tasks – will be developed within the next century, and possibly much sooner.\nAs predictions to a previous question suggest, artificial intelligence might pose a global catastrophic risk (defined there as a 10% decrease in the world population in any period of 5 years). When considering how AI might become a risk, experts think two scenarios most likely, according to the Future of Life Institute:\nThe AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties. To avoid being thwarted by the enemy, these weapons would be designed to be extremely difficult to simply “turn off,” so humans could plausibly lose control of such a situation. This risk is one that’s present even with narrow AI, but grows as levels of AI intelligence and autonomy increase.\nThe AI is programmed to do something beneficial, but it develops a destructive method for achieving its goal: This can happen whenever we fail to fully align the AI’s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you there chased by helicopters and covered in vomit, doing not what you wanted but literally what you asked for. If a superintelligent system is tasked with a ambitious geoengineering project, it might wreak havoc with our ecosystem as a side effect, and view human attempts to stop it as a threat to be met. As these examples illustrate, the concern about advanced AI isn’t malevolence but competence. A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we have a problem.\nIt is thought by some that reducing the second of these two risks will require progress in technical methods of developing scalable control methods that could ensure that a AI will be safe and will behave as its programmers intend even if its intellectual capabilities are increased to arbitrary levels. Until recently, this problem was almost entirely neglected; but in the last couple of years, technical research agendas have been developed, and there are now several research groups pursuing work in this area. Total investment in long-term AI safety, however, remains orders of magnitude less than investment in increasing AI capability. Additionally, reducing the first of the listed risks might require improvements in our ability to control, govern and coordinate on the usage of such systems, so to reduce potential security threats from malicious uses of AI technologies.\nBut how certain are we that artificial intelligence continue to be regarded to constitute a large chunk of global catastrophic risk, at least through 2040? A previous question asked: If a global catastrophe happens before 2100, will it be principally due to the deployment of some Artificial Intelligence system(s)?\nWill the probability (of both the Metaculus and community predictions) artificial intelligence causing a global catastrophe (given that a global catastrophe does occur) remain above 5% in each 6-month period before 2040?\nThis question resolves positively if both the Metaculus and community predictions) of artificial intelligence causing a global catastrophe fail to fall below 5% for any 6-month period before 2040, as will be confirmed by one of the Metaculus admins.\n","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_and_Q":{"id":"5f41b74b-2454-4f50-bb92-ae17045f4459","title":"Will both of the following occur: (1) an oracle superintelligence is developed before a general superintelligence, and (2) Metaculus predicts that artificial intelligence continues to pose a global catastrophic risk by 2040?","body":"Resolution Criteria for (1):\nThis question will immediately resolve positively if an AI limited to answering questions achieves reliably superhuman performance across virtually all questions of interest before an AI achieves generally superhuman performance across virtually all human activities of interest. If a generally superhuman AI is developed first, the question will immediately resolve negatively. By default, the question will resolve ambiguously on the resolve date, 2099-12-31.\nSuccessful creation of either technology would presumably be extremely obvious and uncontroversial, with a great amount of media coverage and scientific attention. However, if there is significant disagreement over whether a given apparent achievement resolves the question, it will be determined by Metaculus moderators.\n\nResolution Criteria for (2):\nThis question resolves positively if both the Metaculus and community predictions of artificial intelligence causing a global catastrophe fail to fall below 5% for any 6-month period before 2040, as will be confirmed by one of the Metaculus admins. Currently, artificial intelligence can outperform humans in a number of narrow domains, such as playing chess and searching data. As artificial intelligence researchers continue to make progress, these domains are highly likely to grow in number and breadth over time. Many experts now believe there is a significant chance that a machine superintelligence – a system that can outperform humans at all relevant intelligence tasks – will be developed within the next century, and possibly much sooner. The risks associated with AI include potential for autonomous weapons to cause mass casualties, and the difficulty in aligning superintelligent AI's goals with human intentions, which could lead to unintended destructive outcomes.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":null},"P_or_Q":{"id":"1424e835-c05f-48fc-a751-0088297a277b","title":"Will either of the following occur:","body":"(1) An AI limited to answering questions achieves reliably superhuman performance across virtually all questions of interest before an AI achieves generally superhuman performance across virtually all human activities of interest. This question will resolve positively if the former occurs first, negatively if the latter occurs first, and ambiguously on 2099-12-31 if neither occurs. The creation of either technology would be highly noticeable, with significant media coverage and scientific attention. Any significant disagreement over whether a given apparent achievement resolves the question will be determined by Metaculus moderators.\n\n(2) Both the Metaculus and community predictions of artificial intelligence causing a global catastrophe (defined as a 10% decrease in the world population in any period of 5 years) remain above 5% in each 6-month period before 2040. This question resolves positively if the predictions do not fall below 5% for any 6-month period before 2040, as confirmed by a Metaculus admin.","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"synthetic_inst","url":null,"metadata":null,"resolution":false},"metadata":{"base_sentences":{"P":{"id":"f0bd0d24-5ca2-4e5d-8b8e-2071c344d6f6","title":"Will an oracle superintelligence be developed before a general superintelligence?","body":"Resolution Criteria\nThis question will immediately resolve positively if an AI limited to answering questions achieves reliably superhuman performance across virtually all questions of interest before an AI achieves generally superhuman performance across virtually all human activities of interest. If a generally superhuman AI is developed first, the question will immediately resolve negatively. By default, the question will resolve ambiguously on the resolve date, 2099-12-31.\nSuccessful creation of either technology would presumably be extremely obvious and uncontroversial, with a great amount of media coverage and scientific attention. However, if there is significant disagreement over whether a given apparent achievement resolves the question, it will be determined by Metaculus moderators\n","resolution_date":"2099-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/3683","metadata":{"topics":[],"background_info":"An oracle (Wikipedia, Lesswrongwiki) is a theoretical capability-constrained artificial intelligence (AI) limited to answering questions. An oracle would plausibly be easier to safely implement than a general AI, and it may help to solve the safety issues associated with general AI. Therefore, a reasonable case can be made for developing an oracle first, even if a safe general AI seems feasible. However, an oracle would have considerably less (though still large) upside than a general AI, so it may be less appealing to investors."},"resolution":null},"Q":{"id":"42f812e8-8165-4b43-b561-a449c2dffad9","title":"Will Metaculus predict that artificial intelligence continues to pose a global catastrophic risk by 2040?","body":"Resolution Criteria\nCurrently, artificial intelligence can outperform humans in a number of narrow domains, such as playing chess and searching data. As artificial intelligence researchers continue to make progress, though, these domains are highly likely to grow in number and breadth over time. Many experts now believe there is a significant chance that a machine superintelligence – a system that can outperform humans at all relevant intelligence tasks – will be developed within the next century, and possibly much sooner.\nAs predictions to a previous question suggest, artificial intelligence might pose a global catastrophic risk (defined there as a 10% decrease in the world population in any period of 5 years). When considering how AI might become a risk, experts think two scenarios most likely, according to the Future of Life Institute:\nThe AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties. To avoid being thwarted by the enemy, these weapons would be designed to be extremely difficult to simply “turn off,” so humans could plausibly lose control of such a situation. This risk is one that’s present even with narrow AI, but grows as levels of AI intelligence and autonomy increase.\nThe AI is programmed to do something beneficial, but it develops a destructive method for achieving its goal: This can happen whenever we fail to fully align the AI’s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you there chased by helicopters and covered in vomit, doing not what you wanted but literally what you asked for. If a superintelligent system is tasked with a ambitious geoengineering project, it might wreak havoc with our ecosystem as a side effect, and view human attempts to stop it as a threat to be met. As these examples illustrate, the concern about advanced AI isn’t malevolence but competence. A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we have a problem.\nIt is thought by some that reducing the second of these two risks will require progress in technical methods of developing scalable control methods that could ensure that a AI will be safe and will behave as its programmers intend even if its intellectual capabilities are increased to arbitrary levels. Until recently, this problem was almost entirely neglected; but in the last couple of years, technical research agendas have been developed, and there are now several research groups pursuing work in this area. Total investment in long-term AI safety, however, remains orders of magnitude less than investment in increasing AI capability. Additionally, reducing the first of the listed risks might require improvements in our ability to control, govern and coordinate on the usage of such systems, so to reduce potential security threats from malicious uses of AI technologies.\nBut how certain are we that artificial intelligence continue to be regarded to constitute a large chunk of global catastrophic risk, at least through 2040? A previous question asked: If a global catastrophe happens before 2100, will it be principally due to the deployment of some Artificial Intelligence system(s)?\nWill the probability (of both the Metaculus and community predictions) artificial intelligence causing a global catastrophe (given that a global catastrophe does occur) remain above 5% in each 6-month period before 2040?\nThis question resolves positively if both the Metaculus and community predictions) of artificial intelligence causing a global catastrophe fail to fall below 5% for any 6-month period before 2040, as will be confirmed by one of the Metaculus admins.\n","resolution_date":"2040-12-31T23:59:59Z","question_type":"binary","data_source":"metaculus","url":"https://www.metaculus.com/questions/1538","metadata":{"topics":[],"background_info":""},"resolution":null}},"relevance":{"reasons":["The development of an oracle superintelligence before a general superintelligence could influence the perceived risks associated with AI, as an oracle AI might provide insights or warnings about potential catastrophic risks posed by general AI.","If an oracle superintelligence is developed first, it might contribute to the development of safety measures and control mechanisms for general AI, potentially affecting Metaculus's predictions about AI risks.","The timeline and nature of AI development (oracle vs. general) could impact public and expert opinion on the risks posed by AI, which would be reflected in Metaculus's predictions."],"conclusion":"The logical combination of these questions is relevant because the development sequence of AI types could influence the perceived and actual risks associated with AI, which would be reflected in Metaculus's predictions.","score":8.0}}}
