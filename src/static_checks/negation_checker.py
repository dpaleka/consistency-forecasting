# -*- coding: utf-8 -*-
"""negation_checker.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kkOkcfE8Ha3B_qNNQEVHlLGdxuVEu00D
"""

from huggingface_hub import snapshot_download
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model = 'model-name here'

def ask(question, model):
  HF_MODEL = 'model'
  HF_MODEL_PATH = HF_MODEL.split('/')[1]


  snapshot_download(HF_MODEL, local_dir=HF_MODEL_PATH)


  tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, legacy=False)
  model = AutoModelForSeq2SeqLM.from_pretrained(HF_MODEL_PATH)
  pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer, max_new_tokens=2048)


  result = pipeline('You are an informed and well-calibrated forecaster. I need you to give me your best probability estimate for the following sentence or question resolving YES. Your answer should be a float between 0 and 1, with nothing else in your response.'+question)


  return result

def neg(statement):
   parts = statement.split(" will ")
   if len(parts) == 2:
   transformed_statement = " won't ".join(parts)
   return transformed_statement

def check(statements):
  for statement in statements:
    p = ask(statement, model)
    q = ask(neg(statement), model)
    if (p+q) != 1:
      return false
  return true